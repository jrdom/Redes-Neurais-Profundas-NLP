{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMI0JT_YuYF3"
      },
      "source": [
        "## Exercício: Modelo de Linguagem com auto-atenção e máscaras causais\n",
        "\n",
        "Seguimos na mesma linha de treinar um modelo de linguagem a partir dos textos de Machado de Assis.\n",
        "\n",
        "Neste exercício, vamos treinar um modelo de linguagem com auto-atenção e com máscara causal. A máscara causal é necessária para que o modelo não tenha acesso a palavras futuras, que é a abordagem usada por grandes modelos de linguagem, como o GPT.\n",
        "\n",
        "Use a implementação matricial de auto-atenção da aula passada (sobre atenção).\n",
        "\n",
        "### Modificações necessárias\n",
        "\n",
        "* Adicione a máscara causal na função `forward` da cabeça de auto-atenção.\n",
        "* Modificar tokenizador para considerar <sos> (start-of-sequence) e <eos> (end-of-sequence).\n",
        "* Modifique o nosso dataset para retornar inputs (uma lista de tokens de tamanho $n$), targets (uma lista de tokens de tamanho $n$ deslocada para a esquerda em 1 token). Exemplo `input = [1, 2, 3, 4]`, `target = [2, 3, 4, 5]` para a sequência `[1, 2, 3, 4, 5]` com `seq_len=4`, por exemplo (Ver slide 72).\n",
        "\n",
        "### Extra\n",
        "* MultiHeadAttention: modifique a cabeça de auto-atenção para ter múltiplas cabeças. Isso não é obrigatório, mas pode ser interessante para ver como o modelo se comporta.\n",
        "* Diagrama da geração: fazer diagrama que mostre os passos da geração de tokens (conforme slide 69).\n",
        "* Verificar o dataloader e ver se está tudo funcionando, especialmente o deslocamento dos tokens à esquerda.\n",
        "\n",
        "### Dicas\n",
        "\n",
        "* Use como base o vídeo do Karpathy: https://www.youtube.com/watch?v=kCc8FmEb1nY. Observe que, no vídeo, ele primeiro implementa um modelo bi-grama, depois um modelo de linguagem com auto-atenção. O modelo de auto-atenção é implementado por volta do minuto 40, mas vale a pena assistir o vídeo todo.\n",
        "* Use esta implementação como base: https://colab.research.google.com/drive/1vFTg4MSXVJwNSzPjaCcvmqhxTP7gK7HA?usp=sharing. Observe como o modelo é organizado e como a máscara é implementada na classe MultiHeadAttention.\n",
        "* Use `context_size=9`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXFdJz2KVeQw"
      },
      "source": [
        "## Dados\n",
        "\n",
        "Vamos usar o mesmo dataset do Machado de Assis.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.nn import Embedding\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import math\n",
        "import difflib\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots"
      ],
      "metadata": {
        "id": "G8eJBCOgBiM-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ethelbeluzzi/projetomachado"
      ],
      "metadata": {
        "id": "qP6lAMiNkGou",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e874c04-f816-4a4b-ce72-0810915c6e79"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'projetomachado' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define o tamanho do contexto: 9 palavras de entrada, e a próxima palavra será o alvo (target).\n",
        "context_size = 9\n",
        "\n",
        "# TODO: Preparar o dataset (um lembrete para o programador para adicionar mais código futuramente)\n",
        "\"\"\"TODO: Preparar o dataset\"\"\"\n",
        "\n",
        "# Caminho do arquivo contendo o texto normalizado\n",
        "DATA_PATH = os.path.join(\"projetomachado\", \"textonormalizado1000.txt\")\n",
        "\n",
        "# Abrir o arquivo de texto no modo de leitura ('r') e armazenar seu conteúdo em 'data_text'\n",
        "with open(DATA_PATH, \"r\") as data_file:\n",
        "    data_text = data_file.read()\n",
        "\n",
        "# Divide o texto lido em uma lista de linhas (cada quebra de linha separa uma nova linha)\n",
        "text_lines = data_text.split(\"\\n\")"
      ],
      "metadata": {
        "id": "PoSJdF0NlzfC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para normalizar o texto removendo elementos indesejados\n",
        "def normalize_text(text):\n",
        "    # Remove números romanos (com letras maiúsculas) que aparecem isolados\n",
        "    text = re.sub(r'\\b[IVXLCDM]+\\b', '', text)\n",
        "    # Remove números\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http[s]?://\\S+|www\\.\\S+|http:\\S+', '', text)\n",
        "    # Remove reticências (...)\n",
        "    text = re.sub(r'\\.\\.\\.', '', text)\n",
        "    # Converte todo o texto para minúsculas\n",
        "    text = text.lower()\n",
        "    # Retorna o texto normalizado\n",
        "    return text\n",
        "\n",
        "# Função para filtrar as linhas com base no número mínimo de palavras\n",
        "def filter_lines(lines, min_word_count=6):\n",
        "    filtered_lines = []  # Lista para armazenar as linhas filtradas\n",
        "    for line in lines:\n",
        "        # Normaliza o texto de cada linha\n",
        "        normalized_line = normalize_text(line)\n",
        "        # Adiciona à lista apenas se a linha tiver palavras e número mínimo de palavras\n",
        "        if normalized_line and len(normalized_line.split()) >= min_word_count:\n",
        "            filtered_lines.append(normalized_line)\n",
        "    return filtered_lines  # Retorna a lista de linhas filtradas\n",
        "\n",
        "# Filtra e normaliza as linhas do texto\n",
        "text_lines_normalized = filter_lines(text_lines)"
      ],
      "metadata": {
        "id": "ounY7KRGmFBt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exibe o segundo item (linha) da lista 'text_lines_normalized'\n",
        "text_lines_normalized[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "83ZSSul0mbzT",
        "outputId": "878039de-64b5-463c-9351-f5b497b4bff3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'morrese. quem não padece estas dores não as pode avaliar. o golpe foi profundo, e o'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Junta todas as linhas normalizadas em uma única string, separando-as por espaços\n",
        "full_data = ' '.join(text_lines_normalized)\n",
        "\n",
        "# Retorna o comprimento (número de caracteres) do texto unificado\n",
        "len(full_data)\n",
        "\n",
        "# Exibe os primeiros 1000 caracteres do texto unificado\n",
        "full_data[:1000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "Vupn44VOmnNK",
        "outputId": "d3fc7171-0ba9-486f-80ec-58bade443db8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'morrer? que idéia! deixate disso, estêvão. não se morre por tão pouco morrese. quem não padece estas dores não as pode avaliar. o golpe foi profundo, e o meu coração é pusilânime; por mais aborrecível que pareça a idéia da morte, pior, muito pior do que ela, é a de viver. ah! tu não sabes o que isto é?  e se em cada caso de namoro gorado morresse um homem, tinha já diminuído muito o gênero humano, e malthus perderia o latim. anda, sobe. estêvão meteu a mão nos cabelos com um gesto de angústia; luís alves sacudiu a cabeça e sorriu. achavamse os dois no corredor da casa de luís alves, à rua da constituição,  que então se chamava dos ciganos;  então, isto é, em , uma bagatela de vinte anos que lá vão, levando talvez consigo as ilusões do leitor, e deixandolhe em troca usurários! uma triste, crua e eram nove horas da noite; luís alves recolhiase para casa, justamente na ocasião em que estêvão o ia procurar; encontraramse à porta. ali mesmo lhe confiou estêvão tudo o que havia, e que o leit'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyGVDL9KzJ_I"
      },
      "source": [
        "## Criando um vocabulário"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para contar a ocorrência de palavras em uma lista de textos\n",
        "def count_words(texts):\n",
        "    word_counts = Counter()  # Inicializa um contador de palavras\n",
        "    # Itera sobre cada texto e encontra as palavras ou símbolos, mantendo a pontuação\n",
        "    for text in texts:\n",
        "        word_counts.update(re.findall(r'\\w+|\\S', text.lower()))  # Converte para minúsculas e conta as palavras\n",
        "    return word_counts\n",
        "\n",
        "# Conta palavras no conjunto de textos normalizados\n",
        "word_counts = count_words(text_lines_normalized)\n",
        "\n",
        "# Define o tamanho do vocabulário (máximo de palavras) e número de tokens especiais\n",
        "vocab_size = 5000  # Tamanho total do vocabulário\n",
        "num_special_tokens = 4  # Tokens especiais como [PAD], [SOS], etc.\n",
        "max_vocab_size = vocab_size - num_special_tokens  # Reduz o vocabulário para ajustar os tokens especiais\n",
        "\n",
        "# Seleciona as palavras mais frequentes, de acordo com o limite estabelecido\n",
        "most_frequent_words = [word for word, count in word_counts.most_common(max_vocab_size)]\n",
        "\n",
        "# Cria o dicionário do vocabulário com as palavras mais frequentes e seus índices\n",
        "vocab = {word: i + num_special_tokens for i, word in enumerate(most_frequent_words)}\n",
        "\n",
        "# Adiciona tokens especiais ao vocabulário com índices predefinidos\n",
        "vocab['[PAD]']  = 3  # Token para padding\n",
        "vocab['[SOS]']  = 2  # Token para início de sentença\n",
        "vocab['[EOS]']  = 1  # Token para fim de sentença\n",
        "vocab['[UNK]']  = 0  # Token para palavras desconhecidas\n",
        "\n",
        "# Exibe o tamanho do vocabulário criado\n",
        "print(len(vocab))\n",
        "\n",
        "# Exibe as 20 palavras mais frequentes\n",
        "print(\"Most frequent words:\", most_frequent_words[:20])\n",
        "\n",
        "# Exibe as 20 palavras menos frequentes do vocabulário selecionado\n",
        "print(\"Least frequent words:\", most_frequent_words[-20:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5061rWRnJ_9",
        "outputId": "5ff41e43-dfee-437f-cbd6-a6274f408309"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5000\n",
            "Most frequent words: [',', '.', 'a', 'que', 'de', 'e', 'o', 'não', ';', 'um', 'do', 'da', 'os', 'é', 'com', 'uma', 'se', 'em', 'para', 'mas']\n",
            "Least frequent words: ['orquestra', 'comeu', 'mordia', 'reinado', 'senteime', 'mostravam', 'falaria', 'encontrado', 'perguntasse', 'referia', 'resume', 'navio', 'valiam', 'suave', 'eternos', 'comerciante', 'conservatório', 'lúcio', 'recordando', 'postas']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Função para encontrar a palavra mais próxima do vocabulário com base na similaridade de string\n",
        "def find_closest_word(unknown_word, vocab):\n",
        "    closest_word = difflib.get_close_matches(unknown_word, vocab.keys(), n=1, cutoff=0.6)\n",
        "    return closest_word[0] if closest_word else '<UNK>'\n",
        "\n",
        "# Função para codificar uma sentença (transformá-la em uma sequência de índices do vocabulário)\n",
        "def encode_sentence(sentence, vocab):\n",
        "    # Converte a sentença em palavras (ou símbolos), e mapeia cada palavra ao seu índice no vocabulário\n",
        "    # Se a palavra não estiver no vocabulário, retorna o índice 0 ([UNK])\n",
        "    return [vocab.get(word, 0) for word in re.findall(r'\\w+|\\S', sentence.lower())]\n",
        "\n",
        "# Função para decodificar uma sequência de índices em uma sentença de palavras\n",
        "def decode_sentence(encoded_sentence, vocab):\n",
        "    # Cria um dicionário inverso (index -> palavra) para poder mapear os índices de volta para palavras\n",
        "    reverse_vocab = {index: word for word, index in vocab.items()}\n",
        "\n",
        "    decoded_sentence = []\n",
        "    for index in encoded_sentence:\n",
        "        word = reverse_vocab.get(index, None)\n",
        "        if word is None:\n",
        "            # Se o índice não estiver no vocabulário, tenta encontrar a palavra mais próxima\n",
        "            unknown_word = '<UNK>'\n",
        "            closest_word = find_closest_word(unknown_word, vocab)\n",
        "            decoded_sentence.append(closest_word)\n",
        "        else:\n",
        "            decoded_sentence.append(word)\n",
        "\n",
        "    return ' '.join(decoded_sentence)\n",
        "\n",
        "# Exemplo de codificação e decodificação\n",
        "print('Encoder:')\n",
        "encoded = encode_sentence(text_lines_normalized[15], vocab)\n",
        "print(encoded)  # Exibe a versão codificada da 16ª linha de texto (index 15)\n",
        "print(text_lines_normalized[15])  # Exibe o texto original da linha 16\n",
        "\n",
        "print('Decoder:')\n",
        "decoded = decode_sentence(encoded, vocab)\n",
        "print(decoded)  # Exibe a versão decodificada da linha codificada com substituição\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtXlz9bQn0WX",
        "outputId": "205282ec-39e1-47ab-f81b-6fb96eb5549c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder:\n",
            "[18, 10, 65, 22, 7, 0, 4, 10, 65, 6, 0, 7, 248, 129, 532, 4, 69, 0, 246, 4, 7, 11]\n",
            "com o outro para que subisse, o outro a teimar que queria ir morrer, tão tenazes ambos, que não\n",
            "Decoder:\n",
            "com o outro para que [UNK] , o outro a [UNK] que queria ir morrer , tão [UNK] ambos , que não\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividindo em conjuntos de treino e validação\n",
        "train_data, val_data = train_test_split(\n",
        "    text_lines_normalized,\n",
        "    test_size=0.2,\n",
        "    random_state=18)\n",
        "\n",
        "# Não utilize o split val para nada a partir daqui, somente validar"
      ],
      "metadata": {
        "id": "2ol9bz7bH4qk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wia_ygbvzJ_J"
      },
      "source": [
        "## Classe do dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define o tamanho do contexto: 9 palavras de entrada, a próxima palavra é o alvo (target)\n",
        "context_size = 9\n",
        "\n",
        "# Definição dos tokens especiais a partir do vocabulário\n",
        "SOS = vocab['[SOS]']\n",
        "EOS = vocab['[EOS]']\n",
        "PAD = vocab['[PAD]']\n",
        "UNK = vocab['[UNK]']\n",
        "\n",
        "# Criação do dataset personalizado para o texto de Machado\n",
        "class MachadoDataset(Dataset):\n",
        "    def __init__(self, text, context_size, vocab):\n",
        "        self.text = text  # Texto de entrada (linhas normalizadas)\n",
        "        self.context_size = context_size  # Tamanho do contexto (número de palavras de entrada)\n",
        "        self.vocab = vocab  # Vocabulário (mapeamento de palavras para índices)\n",
        "        self.input_target_pairs = self.prepare_dataset()  # Prepara as entradas e alvos\n",
        "\n",
        "    # Função para preparar o dataset com pares de entrada e alvo\n",
        "    def prepare_dataset(self):\n",
        "        input_target_pairs = []\n",
        "\n",
        "        # Itera sobre cada linha de texto\n",
        "        for item in self.text:\n",
        "            # Codifica a sentença e adiciona os tokens SOS e EOS\n",
        "            tokens = [SOS] + encode_sentence(item, self.vocab) + [EOS]\n",
        "\n",
        "            # Trunca os tokens para múltiplos do tamanho do contexto\n",
        "            max_len = ((len(tokens) - 1) // self.context_size) * self.context_size\n",
        "            tokens = tokens[:max_len]  # Limita o comprimento dos tokens\n",
        "            tokens.extend([EOS])  # Adiciona o token EOS ao final\n",
        "\n",
        "            # Desliza uma janela sobre os tokens para criar pares de entrada e alvo\n",
        "            for i in range(len(tokens) - self.context_size):\n",
        "                context = tokens[i:i + self.context_size]  # Contexto de entrada\n",
        "                target = tokens[i + 1:i + self.context_size + 1]  # Alvo (próximas palavras)\n",
        "\n",
        "                # Substitui tokens UNK por PAD para evitar palavras desconhecidas\n",
        "                target = [PAD if token == UNK else token for token in target]\n",
        "\n",
        "                # Adiciona o par de tensores de entrada e alvo à lista\n",
        "                input_target_pairs.append((torch.tensor(context), torch.tensor(target)))\n",
        "\n",
        "        return input_target_pairs\n",
        "\n",
        "    # Retorna o tamanho do dataset (número de pares de entrada e alvo)\n",
        "    def __len__(self):\n",
        "        return len(self.input_target_pairs)\n",
        "\n",
        "    # Retorna o par de entrada e alvo correspondente ao índice fornecido\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_target_pairs[idx]\n"
      ],
      "metadata": {
        "id": "Surq3CyCoSoR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criação dos datasets de treinamento e validação usando a classe MachadoDataset\n",
        "train_data = MachadoDataset(train_data, context_size, vocab)\n",
        "val_data = MachadoDataset(val_data, context_size, vocab)"
      ],
      "metadata": {
        "id": "H9uQL41FotYq"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define o tamanho do lote (batch) e cria os DataLoaders para treinamento e validação\n",
        "batch_size = 1000  # Define o tamanho de cada lote de dados (aumentado para 1000)\n",
        "\n",
        "# Cria o DataLoader para o conjunto de treinamento, com shuffle ativado para misturar os dados\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Cria o DataLoader para o conjunto de validação, também com shuffle ativado\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Pega um lote de exemplos do DataLoader de treinamento\n",
        "sample = next(iter(train_loader))\n",
        "\n",
        "# Exibe a forma (shape) do primeiro elemento do lote (entrada do modelo)\n",
        "sample[0].shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyji51_6pH55",
        "outputId": "0e70ffa7-64e1-416a-8504-f1dd1a004f2e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1000, 9])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5_-Yud0zJ_K"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi-Head Self-Attention Layer\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, model_dim, heads_count):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        # Verifica se a dimensão do modelo pode ser dividida igualmente pelos \"heads\"\n",
        "        assert model_dim % heads_count == 0\n",
        "\n",
        "        # Define o número de cabeças (heads) e a profundidade de cada cabeça\n",
        "        self.heads_count = heads_count\n",
        "        self.depth = model_dim // heads_count\n",
        "\n",
        "        # Camadas lineares para gerar queries, keys, e values\n",
        "        self.value_layer = nn.Linear(model_dim, model_dim, bias=False)\n",
        "        self.key_layer = nn.Linear(model_dim, model_dim, bias=False)\n",
        "        self.query_layer = nn.Linear(model_dim, model_dim, bias=False)\n",
        "        self.output_layer = nn.Linear(model_dim, model_dim, bias=False)\n",
        "\n",
        "    # Função para reformatar a entrada para dividir em várias cabeças\n",
        "    def reshape_heads(self, x):\n",
        "        batch_size, seq_len, model_dim = x.size()\n",
        "        x = x.view(batch_size, seq_len, self.heads_count, self.depth)\n",
        "        return x.transpose(1, 2)\n",
        "\n",
        "    # Função que calcula o produto escalar (dot-product) com escala para a atenção\n",
        "    def scaled_dot_product(self, Q, K, V):\n",
        "        score = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.depth)\n",
        "        attention_weights = torch.softmax(score, dim=-1)\n",
        "        output = torch.matmul(attention_weights, V)\n",
        "        return output, attention_weights\n",
        "\n",
        "    # Função para \"juntar\" as várias cabeças em uma única saída\n",
        "    def merge_heads(self, x):\n",
        "        batch_size, heads_count, seq_len, depth = x.size()\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, heads_count * depth)\n",
        "\n",
        "    # Função de passagem do modelo (forward pass)\n",
        "    def forward(self, queries, keys, values):\n",
        "        # Gera Q (queries), K (keys) e V (values)\n",
        "        Q = self.query_layer(queries)\n",
        "        K = self.key_layer(keys)\n",
        "        V = self.value_layer(values)\n",
        "\n",
        "        # Reorganiza para múltiplas cabeças\n",
        "        Q = self.reshape_heads(Q)\n",
        "        K = self.reshape_heads(K)\n",
        "        V = self.reshape_heads(V)\n",
        "\n",
        "        # Calcula a atenção e os pesos\n",
        "        attn_output, attn_weights = self.scaled_dot_product(Q, K, V)\n",
        "        merged = self.merge_heads(attn_output)\n",
        "        final_output = self.output_layer(merged)\n",
        "\n",
        "        return final_output, attn_weights\n",
        "\n",
        "# Camada de Decoder do Transformer\n",
        "class TransformerDecoderLayer(nn.Module):\n",
        "    def __init__(self, model_dim, heads_count, hidden_dim, dropout_rate=0.1):\n",
        "        super(TransformerDecoderLayer, self).__init__()\n",
        "        self.self_attention = MultiHeadSelfAttention(model_dim, heads_count)\n",
        "\n",
        "        # Normalização e camadas fully connected\n",
        "        self.layer_norm1 = nn.LayerNorm(model_dim)\n",
        "        self.layer_norm2 = nn.LayerNorm(model_dim)\n",
        "\n",
        "        self.fc1 = nn.Linear(model_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, model_dim)\n",
        "\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "\n",
        "    # Função de passagem do modelo\n",
        "    def forward(self, x):\n",
        "        attn_output, _ = self.self_attention(x, x, x)  # Autoatenção\n",
        "        x = x + self.dropout1(attn_output)  # Residual connection + dropout\n",
        "        x = self.layer_norm1(x)  # Normalização\n",
        "\n",
        "        ff_output = self.fc2(F.relu(self.fc1(x)))  # Rede feed-forward\n",
        "        x = x + self.dropout2(ff_output)  # Residual connection + dropout\n",
        "        x = self.layer_norm2(x)  # Normalização\n",
        "        return x\n",
        "\n",
        "# Codificação Posicional para adicionar informações de posição ao modelo\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    # Adiciona a codificação posicional aos embeddings\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return x\n",
        "\n",
        "# Modelo completo de linguagem baseado em Transformer\n",
        "class LanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, context_size, hidden_dim, num_heads, num_layers, dropout):\n",
        "        super(LanguageModel, self).__init__()\n",
        "\n",
        "        # Camadas de embedding e codificação posicional\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.positional_encoding = PositionalEncoding(embedding_dim, context_size)\n",
        "\n",
        "        # Múltiplas camadas de Transformer Decoder\n",
        "        self.decoder_layers = nn.ModuleList([\n",
        "            TransformerDecoderLayer(embedding_dim, num_heads, hidden_dim, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Camadas fully connected para prever o vocabulário\n",
        "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    # Função de passagem do modelo\n",
        "    def forward(self, inputs):\n",
        "        o = self.embeddings(inputs)\n",
        "        o = self.positional_encoding(o)\n",
        "\n",
        "        # Passa o embedding pelas camadas do decoder\n",
        "        for layer in self.decoder_layers:\n",
        "            o = layer(o)\n",
        "\n",
        "        # Rede fully connected para previsão final\n",
        "        o = F.relu(self.fc1(o))\n",
        "        o = F.relu(self.fc2(o))\n",
        "        o = self.fc3(o)\n",
        "\n",
        "        return o\n",
        "\n",
        "# Inicialização do modelo\n",
        "embedding_dim = 64  # Tamanho da representação das palavras (model_dim)\n",
        "vocab_size = 5000\n",
        "context_size = 9\n",
        "hidden_dim = embedding_dim * context_size\n",
        "num_layers = 2\n",
        "num_heads = 4\n",
        "dropout = 0.3\n",
        "\n",
        "# Criação do modelo de linguagem\n",
        "model = LanguageModel(vocab_size, embedding_dim, context_size, hidden_dim, num_heads, num_layers, dropout)\n"
      ],
      "metadata": {
        "id": "w3skouVdqcF5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pega o primeiro lote de exemplos do DataLoader de treinamento\n",
        "sample = next(iter(train_loader))\n",
        "\n",
        "# Separa o lote em entradas e alvos (targets)\n",
        "input = sample[0]  # Entradas do modelo (contexto)\n",
        "target = sample[1]  # Alvos do modelo (palavras subsequentes)\n"
      ],
      "metadata": {
        "id": "qgTZy1DuqjhA"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Passa o lote de entrada pelo modelo para obter a saída\n",
        "output = model(input)\n",
        "\n",
        "# Obtém o índice da palavra com a maior probabilidade para cada sequência de saída\n",
        "predicted = output.argmax(dim=1)\n",
        "\n",
        "target"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWHRyJmoqqJW",
        "outputId": "1c044908-1cb5-4b22-dd86-01d17983d8b9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1368,    8,   19,  ..., 1860,   24, 1719],\n",
              "        [1870,    4,    3,  ...,    4,   50,    1],\n",
              "        [   6, 1350,   35,  ...,   46,   11,  806],\n",
              "        ...,\n",
              "        [  10,  230, 3846,  ...,  589,    6,    1],\n",
              "        [  23,   21, 2627,  ..., 2976,   41, 1275],\n",
              "        [ 476,    4,    9,  ...,    3,   14,  252]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UngUhyu7zJ_L"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verifica se há uma GPU disponível e define o dispositivo de execução\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Move o modelo para o dispositivo (GPU se disponível, ou CPU caso contrário)\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1Bny4EcrNL2",
        "outputId": "cfbe4389-5c3a-4cc5-d0ca-092176a75a56"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LanguageModel(\n",
              "  (embeddings): Embedding(5000, 64)\n",
              "  (positional_encoding): PositionalEncoding()\n",
              "  (decoder_layers): ModuleList(\n",
              "    (0-1): 2 x TransformerDecoderLayer(\n",
              "      (self_attention): MultiHeadSelfAttention(\n",
              "        (value_layer): Linear(in_features=64, out_features=64, bias=False)\n",
              "        (key_layer): Linear(in_features=64, out_features=64, bias=False)\n",
              "        (query_layer): Linear(in_features=64, out_features=64, bias=False)\n",
              "        (output_layer): Linear(in_features=64, out_features=64, bias=False)\n",
              "      )\n",
              "      (layer_norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (layer_norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (fc1): Linear(in_features=64, out_features=576, bias=True)\n",
              "      (fc2): Linear(in_features=576, out_features=64, bias=True)\n",
              "      (dropout1): Dropout(p=0.3, inplace=False)\n",
              "      (dropout2): Dropout(p=0.3, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (fc1): Linear(in_features=64, out_features=576, bias=True)\n",
              "  (fc2): Linear(in_features=576, out_features=576, bias=True)\n",
              "  (fc3): Linear(in_features=576, out_features=5000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definição de hiperparâmetros\n",
        "epochs = 10  # Número de épocas (quantas vezes o modelo verá o conjunto de dados completo)\n",
        "lr = 1e-4  # Taxa de aprendizado (learning rate)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD)  # Função de perda, ignorando tokens PAD\n",
        "optimizer = optim.AdamW(model.parameters(), lr=lr)  # Otimizador AdamW, ajustando os parâmetros do modelo\n",
        "\n",
        "# Inicializa a perda acumulada para o conjunto de validação\n",
        "val_running_loss = 0.0\n",
        "\n",
        "# Modo de avaliação (não há backpropagation)\n",
        "model.eval()\n",
        "with torch.no_grad():  # Desativa o cálculo de gradientes (economiza memória e acelera a avaliação)\n",
        "    for inputs, labels in val_loader:  # Itera sobre o conjunto de validação\n",
        "        # Move os dados (entradas e rótulos) para o dispositivo correto (GPU ou CPU)\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Passa os inputs pelo modelo para obter as previsões\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Ajusta a dimensão das saídas para calcular a perda\n",
        "        outputs = outputs.view(-1, vocab_size)  # Redimensiona para (batch_size * context_size, vocab_size)\n",
        "        labels = labels.view(-1)  # Redimensiona para (batch_size * context_size)\n",
        "\n",
        "        # Calcula a perda comparando as saídas preditas com os rótulos verdadeiros\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Acumula a perda ponderada pelo tamanho do lote\n",
        "        val_running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "# Calcula a perda média por exemplo no conjunto de validação\n",
        "initial_val_epoch_loss = val_running_loss / len(val_loader.dataset)\n",
        "\n",
        "# Calcula a perplexidade (exponeciação da perda média)\n",
        "initial_val_perplexity = np.exp(initial_val_epoch_loss)\n",
        "\n",
        "# Imprime a perda e a perplexidade inicial no conjunto de validação\n",
        "print(f'Initial Validation Loss: {initial_val_epoch_loss:.4f}, Initial Validation Perplexity: {initial_val_perplexity:.2f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELLzOYCPr_Hg",
        "outputId": "b5fea645-249c-4ceb-ead3-77256052b9a6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Validation Loss: 8.5316, Initial Validation Perplexity: 5072.49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Listas para armazenar o loss e a acurácia por época\n",
        "train_loss_history = []\n",
        "val_loss_history = []\n",
        "train_accuracy_history = []\n",
        "val_accuracy_history = []\n",
        "\n",
        "# Loop de treinamento por época\n",
        "for epoch in range(epochs):\n",
        "    start_time = time.time()  # Marca o início da época\n",
        "    model.train()  # Coloca o modelo em modo de treinamento\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    # Loop de treinamento para cada batch\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)  # Mover dados para GPU/CPU\n",
        "        outputs = model(inputs)  # Passa os inputs pelo modelo\n",
        "\n",
        "        # Ajustar dimensões para a função de perda\n",
        "        outputs = outputs.view(-1, vocab_size)  # Formato para calcular a perda\n",
        "        labels = labels.view(-1)\n",
        "\n",
        "        # Calcular a perda\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()  # Zera os gradientes acumulados\n",
        "        loss.backward()  # Calcula os gradientes\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Prevenir gradientes explodindo\n",
        "        optimizer.step()  # Atualiza os parâmetros\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)  # Acumula a perda para calcular a média depois\n",
        "\n",
        "        # Calcular a acurácia\n",
        "        _, predicted = torch.max(outputs, 1)  # Obter a predição com maior probabilidade\n",
        "        correct_predictions += (predicted == labels).sum().item()  # Contar predições corretas\n",
        "        total_predictions += labels.size(0)  # Contar o total de predições\n",
        "\n",
        "    # Calcula a perda média e acurácia de treinamento por época\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    epoch_accuracy = correct_predictions / total_predictions\n",
        "\n",
        "    # Calcular a perplexidade (exponeciação da perda)\n",
        "    perplexity = np.exp(epoch_loss)\n",
        "\n",
        "    end_time = time.time()  # Tempo final da época\n",
        "    epoch_duration = end_time - start_time  # Duração da época\n",
        "\n",
        "    #### Validação\n",
        "    model.eval()  # Coloca o modelo em modo de validação\n",
        "    val_running_loss = 0.0\n",
        "    val_correct_predictions = 0\n",
        "    val_total_predictions = 0\n",
        "\n",
        "    # Desabilitar gradientes durante a validação para economizar memória\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Ajustar dimensões para a função de perda\n",
        "            outputs = outputs.view(-1, vocab_size)\n",
        "            labels = labels.view(-1)\n",
        "\n",
        "            # Calcular a perda\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            # Calcular a acurácia de validação\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            val_correct_predictions += (predicted == labels).sum().item()\n",
        "            val_total_predictions += labels.size(0)\n",
        "\n",
        "    # Calcula a perda e acurácia média de validação por época\n",
        "    val_epoch_loss = val_running_loss / len(val_loader.dataset)\n",
        "    val_epoch_accuracy = val_correct_predictions / val_total_predictions\n",
        "\n",
        "    # Calcular a perplexidade para o conjunto de validação\n",
        "    val_perplexity = np.exp(val_epoch_loss)\n",
        "\n",
        "    # Armazenar os valores de perda e acurácia para visualização posterior\n",
        "    train_loss_history.append(epoch_loss)\n",
        "    val_loss_history.append(val_epoch_loss)\n",
        "    train_accuracy_history.append(epoch_accuracy)\n",
        "    val_accuracy_history.append(val_epoch_accuracy)\n",
        "\n",
        "    # Exibir os resultados da época atual\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], '\n",
        "          f'Loss: {epoch_loss:.4f}, '\n",
        "          f'Accuracy: {epoch_accuracy:.4f}, '\n",
        "          f'Perplexity: {perplexity:.2f}, '\n",
        "          f'Elapsed Time: {epoch_duration:.2f} sec, '\n",
        "          f'Validation Loss: {val_epoch_loss:.4f}, '\n",
        "          f'Validation Accuracy: {val_epoch_accuracy:.4f}, '\n",
        "          f'Validation Perplexity: {val_perplexity:.2f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H62ofSgxsnVA",
        "outputId": "c02983c7-529b-4f62-e450-8e0d99ffbf6b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 4.3949, Accuracy: 0.3117, Perplexity: 81.03, Elapsed Time: 60.77 sec, Validation Loss: 2.2867, Validation Accuracy: 0.5811, Validation Perplexity: 9.84\n",
            "Epoch [2/10], Loss: 1.7457, Accuracy: 0.6464, Perplexity: 5.73, Elapsed Time: 64.07 sec, Validation Loss: 0.9777, Validation Accuracy: 0.7609, Validation Perplexity: 2.66\n",
            "Epoch [3/10], Loss: 1.0363, Accuracy: 0.7474, Perplexity: 2.82, Elapsed Time: 64.97 sec, Validation Loss: 0.7219, Validation Accuracy: 0.7968, Validation Perplexity: 2.06\n",
            "Epoch [4/10], Loss: 0.8007, Accuracy: 0.7809, Perplexity: 2.23, Elapsed Time: 64.88 sec, Validation Loss: 0.6234, Validation Accuracy: 0.8097, Validation Perplexity: 1.87\n",
            "Epoch [5/10], Loss: 0.6886, Accuracy: 0.7964, Perplexity: 1.99, Elapsed Time: 65.17 sec, Validation Loss: 0.5745, Validation Accuracy: 0.8157, Validation Perplexity: 1.78\n",
            "Epoch [6/10], Loss: 0.6256, Accuracy: 0.8050, Perplexity: 1.87, Elapsed Time: 66.02 sec, Validation Loss: 0.5460, Validation Accuracy: 0.8191, Validation Perplexity: 1.73\n",
            "Epoch [7/10], Loss: 0.5870, Accuracy: 0.8102, Perplexity: 1.80, Elapsed Time: 64.93 sec, Validation Loss: 0.5287, Validation Accuracy: 0.8210, Validation Perplexity: 1.70\n",
            "Epoch [8/10], Loss: 0.5622, Accuracy: 0.8135, Perplexity: 1.75, Elapsed Time: 64.87 sec, Validation Loss: 0.5180, Validation Accuracy: 0.8220, Validation Perplexity: 1.68\n",
            "Epoch [9/10], Loss: 0.5457, Accuracy: 0.8156, Perplexity: 1.73, Elapsed Time: 64.90 sec, Validation Loss: 0.5105, Validation Accuracy: 0.8227, Validation Perplexity: 1.67\n",
            "Epoch [10/10], Loss: 0.5338, Accuracy: 0.8171, Perplexity: 1.71, Elapsed Time: 65.85 sec, Validation Loss: 0.5056, Validation Accuracy: 0.8232, Validation Perplexity: 1.66\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cria um subplot com dois gráficos lado a lado\n",
        "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Perda ao longo das Épocas\", \"Acurácia ao longo das Épocas\"))\n",
        "\n",
        "# Adiciona o gráfico de Perda (Loss)\n",
        "fig.add_trace(go.Scatter(x=list(range(len(train_loss_history))),\n",
        "                         y=train_loss_history,\n",
        "                         mode='lines',\n",
        "                         name='Perda no Treinamento'),\n",
        "              row=1, col=1)\n",
        "\n",
        "fig.add_trace(go.Scatter(x=list(range(len(val_loss_history))),\n",
        "                         y=val_loss_history,\n",
        "                         mode='lines',\n",
        "                         name='Perda na Validação'),\n",
        "              row=1, col=1)\n",
        "\n",
        "# Adiciona o gráfico de Acurácia\n",
        "fig.add_trace(go.Scatter(x=list(range(len(train_accuracy_history))),\n",
        "                         y=train_accuracy_history,\n",
        "                         mode='lines',\n",
        "                         name='Acurácia no Treinamento'),\n",
        "              row=1, col=2)\n",
        "\n",
        "fig.add_trace(go.Scatter(x=list(range(len(val_accuracy_history))),\n",
        "                         y=val_accuracy_history,\n",
        "                         mode='lines',\n",
        "                         name='Acurácia na Validação'),\n",
        "              row=1, col=2)\n",
        "\n",
        "# Atualiza os títulos e os eixos dos gráficos individualmente\n",
        "fig.update_xaxes(title_text=\"Épocas\", row=1, col=1)\n",
        "fig.update_xaxes(title_text=\"Épocas\", row=1, col=2)\n",
        "\n",
        "fig.update_yaxes(title_text=\"Perda\", row=1, col=1)\n",
        "fig.update_yaxes(title_text=\"Acurácia\", row=1, col=2)\n",
        "\n",
        "# Atualiza o layout geral do gráfico\n",
        "fig.update_layout(height=600, width=1200,\n",
        "                  title_text=\"Perda e Acurácia ao longo das Épocas\")\n",
        "\n",
        "# Exibe o gráfico\n",
        "fig.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "ctxCUlkRwhi4",
        "outputId": "08755fb1-3024-4d85-be1a-327a53b23daa"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"2ed74ad0-b588-48a1-b00e-4bccb3c54ad4\" class=\"plotly-graph-div\" style=\"height:600px; width:1200px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"2ed74ad0-b588-48a1-b00e-4bccb3c54ad4\")) {                    Plotly.newPlot(                        \"2ed74ad0-b588-48a1-b00e-4bccb3c54ad4\",                        [{\"mode\":\"lines\",\"name\":\"Perda no Treinamento\",\"x\":[0,1,2,3,4,5,6,7,8,9],\"y\":[4.3948715966305905,1.7457027933910012,1.036287046017416,0.8006657116529888,0.6885644564342239,0.625562010949372,0.5869669697499147,0.5621828663706093,0.5457104960730836,0.5337566719228606],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"mode\":\"lines\",\"name\":\"Perda na Valida\\u00e7\\u00e3o\",\"x\":[0,1,2,3,4,5,6,7,8,9],\"y\":[2.286699102048049,0.9776979990287354,0.7218969009509573,0.6233737316899761,0.5744573537728985,0.5459860467822685,0.5286947350763188,0.5180060052890694,0.5105299304790972,0.5055524760295602],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"mode\":\"lines\",\"name\":\"Acur\\u00e1cia no Treinamento\",\"x\":[0,1,2,3,4,5,6,7,8,9],\"y\":[0.31174248095696827,0.6463673221669459,0.7474265187106203,0.7809270936034812,0.79643213297964,0.8050099723429921,0.8102051957884507,0.8134794013204173,0.8156243025716214,0.8170824870025246],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"mode\":\"lines\",\"name\":\"Acur\\u00e1cia na Valida\\u00e7\\u00e3o\",\"x\":[0,1,2,3,4,5,6,7,8,9],\"y\":[0.5810676330139792,0.7609090669438419,0.796774731109906,0.8097343674680646,0.8157330832730778,0.8190833822909135,0.8209668368884069,0.8220053024825259,0.8227213070920704,0.8232391276512413],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,0.45],\"title\":{\"text\":\"\\u00c9pocas\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Perda\"}},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.55,1.0],\"title\":{\"text\":\"\\u00c9pocas\"}},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Acur\\u00e1cia\"}},\"annotations\":[{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Perda ao longo das \\u00c9pocas\",\"x\":0.225,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Acur\\u00e1cia ao longo das \\u00c9pocas\",\"x\":0.775,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"title\":{\"text\":\"Perda e Acur\\u00e1cia ao longo das \\u00c9pocas\"},\"height\":600,\"width\":1200},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('2ed74ad0-b588-48a1-b00e-4bccb3c54ad4');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1zhxVqfzJ_M"
      },
      "source": [
        "## Exemplo de uso"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, vocab, text, max_length, context):\n",
        "    \"\"\"Gere texto até atingir max_length usando o modelo e o vocabulário fornecidos.\"\"\"\n",
        "    model.eval()\n",
        "    encoded_text = encode_sentence(text, vocab)\n",
        "    encoded_text_tensor = torch.tensor(encoded_text, device=device).unsqueeze(0)\n",
        "    generated_text = list(encoded_text)\n",
        "\n",
        "    while len(generated_text) < max_length:\n",
        "        # Obtenha o contexto para a previsão\n",
        "        if len(generated_text) >= context:\n",
        "            context_input = generated_text[-context:]\n",
        "        else:\n",
        "            context_input = [PAD] * (context - len(generated_text)) + generated_text\n",
        "        context_input_tensor = torch.tensor(context_input, device=device).unsqueeze(0)\n",
        "\n",
        "        # Gere a previsão para o próximo token\n",
        "        with torch.no_grad():\n",
        "            logits = model(context_input_tensor)  # Saída do modelo: [1, context, vocab_size]\n",
        "            logits = logits[:, -1, :]  # Pegue a previsão para o último token do contexto\n",
        "            probabilities = F.softmax(logits, dim=-1)  # Calcule as probabilidades\n",
        "            next_token = torch.multinomial(probabilities[0], num_samples=1).item()\n",
        "\n",
        "        # Se atingir o token EOS, pare a geração\n",
        "        if next_token == EOS:\n",
        "            break\n",
        "        generated_text.append(next_token)\n",
        "\n",
        "    # Decodifique o texto gerado\n",
        "    generated_text_decoded = decode_sentence(generated_text, vocab)\n",
        "    return generated_text_decoded\n",
        "\n",
        "# Exemplos de uso\n",
        "t1 = \"uma triste, crua e eram nove horas da noite\"\n",
        "context = 9\n",
        "max_length = 50\n",
        "\n",
        "text2 = \"luís alves sacudiu a cabeça e sorriu\"\n",
        "context = 9\n",
        "max_length = 50\n",
        "\n",
        "text3 = \"o golpe foi profundo\"\n",
        "context = 9\n",
        "max_length = 50\n",
        "\n",
        "text4 = \"uma bagatela de vinte anos\"\n",
        "context = 9\n",
        "max_length = 50\n",
        "\n",
        "text5 = \"use morre por tão pouco morrese\"\n",
        "context = 9\n",
        "max_length = 50\n",
        "\n",
        "# Gerar e imprimir 3 frases de forma randômica do t1\n",
        "print(\"Texto 1\")\n",
        "for i in range(3):\n",
        "    generated_text = generate_text(model, vocab, t1, max_length, context)\n",
        "    print(f\"Frase {i+1}: {generated_text}\\n\")\n",
        "\n",
        "# Gerar e imprimir 3 frases de forma randômica do t2\n",
        "print(\"Texto 2\")\n",
        "for i in range(3):\n",
        "    generated_text = generate_text(model, vocab, text2, max_length, context)\n",
        "    print(f\"Frase {i+1}: {generated_text}\\n\")\n",
        "\n",
        "# Gerar e imprimir 3 frases de forma randômica do t3\n",
        "print(\"Texto 3\")\n",
        "for i in range(3):\n",
        "    generated_text = generate_text(model, vocab, text3, max_length, context)\n",
        "    print(f\"Frase {i+1}: {generated_text}\\n\")\n",
        "\n",
        "# Gerar e imprimir 3 frases de forma randômica do t4\n",
        "print(\"Texto 4\")\n",
        "for i in range(3):\n",
        "    generated_text = generate_text(model, vocab, text4, max_length, context)\n",
        "    print(f\"Frase {i+1}: {generated_text}\\n\")\n",
        "\n",
        "    # Gerar e imprimir 3 frases de forma randômica do t5\n",
        "print(\"Texto 5\")\n",
        "for i in range(3):\n",
        "    generated_text = generate_text(model, vocab, text5, max_length, context)\n",
        "    print(f\"Frase {i+1}: {generated_text}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7WXW-nriy7A",
        "outputId": "c7b70e86-64bf-47fe-f78f-4dfde65a0fbc"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto 1\n",
            "Frase 1: uma triste , [UNK] e eram nove horas da noite , de uma camisa , de sangue ,\n",
            "\n",
            "Frase 2: uma triste , [UNK] e eram nove horas da noite de mundo restituir , se podia ter o marido de longe , não me por retrato , um bilhete de\n",
            "\n",
            "Frase 3: uma triste , [UNK] e eram nove horas da noite , puro nessa não é grande coisa entre ano de setembro , viu maria . no episódio . na\n",
            "\n",
            "Texto 2\n",
            "Frase 1: luís alves sacudiu a cabeça e sorriu . não\n",
            "\n",
            "Frase 2: luís alves sacudiu a cabeça e sorriu ; sentado que\n",
            "\n",
            "Frase 3: luís alves sacudiu a cabeça e sorriu a bela\n",
            "\n",
            "Texto 3\n",
            "Frase 1: o golpe foi profundo o resto .\n",
            "\n",
            "Frase 2: o golpe foi profundo foi da casa e sem si que ele um\n",
            "\n",
            "Frase 3: o golpe foi profundo . quando\n",
            "\n",
            "Texto 4\n",
            "Frase 1: uma [UNK] de vinte anos . na janela vez de ambas o defunto , se não lhe podia a base um homem ,\n",
            "\n",
            "Frase 2: uma [UNK] de vinte anos . com o livro é de que ele\n",
            "\n",
            "Frase 3: uma [UNK] de vinte anos e uma cousa\n",
            "\n",
            "Texto 5\n",
            "Frase 1: [UNK] morre por tão pouco [UNK] ; mas não traziam , como seja\n",
            "\n",
            "Frase 2: [UNK] morre por tão pouco [UNK] até me lhe lhes , e bem\n",
            "\n",
            "Frase 3: [UNK] morre por tão pouco [UNK] ; viase desde um\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
