{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMI0JT_YuYF3"
      },
      "source": [
        "## Exercício: Modelo de Linguagem com auto-atenção e máscaras causais com ajuste fino usando LoRA\n",
        "\n",
        "**Proposta**\n",
        "\n",
        "Este projeto visa realizar um pré-treino de um modelo decoder em português, com o objetivo de gerar textos no estilo de Machado de Assis. O modelo será refinado em duas etapas: uma com todos os pesos atualizados e outra utilizando a técnica LoRA, na qual apenas as matrizes A e B das camadas lineares e embeddings são ajustadas.\n",
        "\n",
        "**Método**\n",
        "\n",
        "- **Dados**: Conjunto de contos de Machado de Assis.\n",
        "- **Modelo**: Transformer com multi-head attention e máscara causal. Durante o refinamento, a técnica LoRA será aplicada em uma das versões, com atualização apenas das matrizes A e B, enquanto a outra versão atualizará todos os pesos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXFdJz2KVeQw"
      },
      "source": [
        "## Dados\n",
        "\n",
        "Vamos usar o mesmo dataset do Machado de Assis.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.nn import Embedding\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import math\n",
        "import difflib\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots"
      ],
      "metadata": {
        "id": "G8eJBCOgBiM-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ethelbeluzzi/projetomachado"
      ],
      "metadata": {
        "id": "qP6lAMiNkGou",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fef3f94f-944e-4274-9f71-085709bba47a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'projetomachado'...\n",
            "remote: Enumerating objects: 65, done.\u001b[K\n",
            "remote: Counting objects: 100% (65/65), done.\u001b[K\n",
            "remote: Compressing objects: 100% (61/61), done.\u001b[K\n",
            "remote: Total 65 (delta 24), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (65/65), 7.21 MiB | 3.45 MiB/s, done.\n",
            "Resolving deltas: 100% (24/24), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define o tamanho do contexto: 9 palavras de entrada, e a próxima palavra será o alvo (target).\n",
        "context_size = 9\n",
        "\n",
        "# TODO: Preparar o dataset (um lembrete para o programador para adicionar mais código futuramente)\n",
        "\"\"\"TODO: Preparar o dataset\"\"\"\n",
        "\n",
        "# Caminho do arquivo contendo o texto normalizado\n",
        "DATA_PATH = os.path.join(\"projetomachado\", \"textonormalizado1000.txt\")\n",
        "\n",
        "# Abrir o arquivo de texto no modo de leitura ('r') e armazenar seu conteúdo em 'data_text'\n",
        "with open(DATA_PATH, \"r\") as data_file:\n",
        "    data_text = data_file.read()\n",
        "\n",
        "# Divide o texto lido em uma lista de linhas (cada quebra de linha separa uma nova linha)\n",
        "text_lines = data_text.split(\"\\n\")"
      ],
      "metadata": {
        "id": "PoSJdF0NlzfC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para normalizar o texto removendo elementos indesejados\n",
        "def normalize_text(text):\n",
        "    # Remove números romanos (com letras maiúsculas) que aparecem isolados\n",
        "    text = re.sub(r'\\b[IVXLCDM]+\\b', '', text)\n",
        "    # Remove números\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http[s]?://\\S+|www\\.\\S+|http:\\S+', '', text)\n",
        "    # Remove reticências (...)\n",
        "    text = re.sub(r'\\.\\.\\.', '', text)\n",
        "    # Converte todo o texto para minúsculas\n",
        "    text = text.lower()\n",
        "    # Retorna o texto normalizado\n",
        "    return text\n",
        "\n",
        "# Função para filtrar as linhas com base no número mínimo de palavras\n",
        "def filter_lines(lines, min_word_count=6):\n",
        "    filtered_lines = []  # Lista para armazenar as linhas filtradas\n",
        "    for line in lines:\n",
        "        # Normaliza o texto de cada linha\n",
        "        normalized_line = normalize_text(line)\n",
        "        # Adiciona à lista apenas se a linha tiver palavras e número mínimo de palavras\n",
        "        if normalized_line and len(normalized_line.split()) >= min_word_count:\n",
        "            filtered_lines.append(normalized_line)\n",
        "    return filtered_lines  # Retorna a lista de linhas filtradas\n",
        "\n",
        "# Filtra e normaliza as linhas do texto\n",
        "text_lines_normalized = filter_lines(text_lines)"
      ],
      "metadata": {
        "id": "ounY7KRGmFBt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exibe o segundo item (linha) da lista 'text_lines_normalized'\n",
        "text_lines_normalized[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "83ZSSul0mbzT",
        "outputId": "2985a9a3-f07d-49f8-8d34-99adc0446830"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'morrese. quem não padece estas dores não as pode avaliar. o golpe foi profundo, e o'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Junta todas as linhas normalizadas em uma única string, separando-as por espaços\n",
        "full_data = ' '.join(text_lines_normalized)\n",
        "\n",
        "# Retorna o comprimento (número de caracteres) do texto unificado\n",
        "len(full_data)\n",
        "\n",
        "# Exibe os primeiros 1000 caracteres do texto unificado\n",
        "full_data[:1000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "Vupn44VOmnNK",
        "outputId": "669f92eb-a9a9-4d57-8fe2-f7f4b7615ef1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'morrer? que idéia! deixate disso, estêvão. não se morre por tão pouco morrese. quem não padece estas dores não as pode avaliar. o golpe foi profundo, e o meu coração é pusilânime; por mais aborrecível que pareça a idéia da morte, pior, muito pior do que ela, é a de viver. ah! tu não sabes o que isto é?  e se em cada caso de namoro gorado morresse um homem, tinha já diminuído muito o gênero humano, e malthus perderia o latim. anda, sobe. estêvão meteu a mão nos cabelos com um gesto de angústia; luís alves sacudiu a cabeça e sorriu. achavamse os dois no corredor da casa de luís alves, à rua da constituição,  que então se chamava dos ciganos;  então, isto é, em , uma bagatela de vinte anos que lá vão, levando talvez consigo as ilusões do leitor, e deixandolhe em troca usurários! uma triste, crua e eram nove horas da noite; luís alves recolhiase para casa, justamente na ocasião em que estêvão o ia procurar; encontraramse à porta. ali mesmo lhe confiou estêvão tudo o que havia, e que o leit'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyGVDL9KzJ_I"
      },
      "source": [
        "## Criando um vocabulário"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para contar a ocorrência de palavras em uma lista de textos\n",
        "def count_words(texts):\n",
        "    word_counts = Counter()  # Inicializa um contador de palavras\n",
        "    # Itera sobre cada texto e encontra as palavras ou símbolos, mantendo a pontuação\n",
        "    for text in texts:\n",
        "        word_counts.update(re.findall(r'\\w+|\\S', text.lower()))  # Converte para minúsculas e conta as palavras\n",
        "    return word_counts\n",
        "\n",
        "# Conta palavras no conjunto de textos normalizados\n",
        "word_counts = count_words(text_lines_normalized)\n",
        "\n",
        "# Define o tamanho do vocabulário (máximo de palavras) e número de tokens especiais\n",
        "vocab_size = 5000  # Tamanho total do vocabulário\n",
        "num_special_tokens = 4  # Tokens especiais como [PAD], [SOS], etc.\n",
        "max_vocab_size = vocab_size - num_special_tokens  # Reduz o vocabulário para ajustar os tokens especiais\n",
        "\n",
        "# Seleciona as palavras mais frequentes, de acordo com o limite estabelecido\n",
        "most_frequent_words = [word for word, count in word_counts.most_common(max_vocab_size)]\n",
        "\n",
        "# Cria o dicionário do vocabulário com as palavras mais frequentes e seus índices\n",
        "vocab = {word: i + num_special_tokens for i, word in enumerate(most_frequent_words)}\n",
        "\n",
        "# Adiciona tokens especiais ao vocabulário com índices predefinidos\n",
        "vocab['[PAD]']  = 3  # Token para padding\n",
        "vocab['[SOS]']  = 2  # Token para início de sentença\n",
        "vocab['[EOS]']  = 1  # Token para fim de sentença\n",
        "vocab['[UNK]']  = 0  # Token para palavras desconhecidas\n",
        "\n",
        "# Exibe o tamanho do vocabulário criado\n",
        "print(len(vocab))\n",
        "\n",
        "# Exibe as 20 palavras mais frequentes\n",
        "print(\"Most frequent words:\", most_frequent_words[:20])\n",
        "\n",
        "# Exibe as 20 palavras menos frequentes do vocabulário selecionado\n",
        "print(\"Least frequent words:\", most_frequent_words[-20:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5061rWRnJ_9",
        "outputId": "63472c5d-7a7e-43ac-b750-632057cb0227"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5000\n",
            "Most frequent words: [',', '.', 'a', 'que', 'de', 'e', 'o', 'não', ';', 'um', 'do', 'da', 'os', 'é', 'com', 'uma', 'se', 'em', 'para', 'mas']\n",
            "Least frequent words: ['orquestra', 'comeu', 'mordia', 'reinado', 'senteime', 'mostravam', 'falaria', 'encontrado', 'perguntasse', 'referia', 'resume', 'navio', 'valiam', 'suave', 'eternos', 'comerciante', 'conservatório', 'lúcio', 'recordando', 'postas']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Função para encontrar a palavra mais próxima do vocabulário com base na similaridade de string\n",
        "def find_closest_word(unknown_word, vocab):\n",
        "    closest_word = difflib.get_close_matches(unknown_word, vocab.keys(), n=1, cutoff=0.6)\n",
        "    return closest_word[0] if closest_word else '<UNK>'\n",
        "\n",
        "# Função para codificar uma sentença (transformá-la em uma sequência de índices do vocabulário)\n",
        "def encode_sentence(sentence, vocab):\n",
        "    # Converte a sentença em palavras (ou símbolos), e mapeia cada palavra ao seu índice no vocabulário\n",
        "    # Se a palavra não estiver no vocabulário, retorna o índice 0 ([UNK])\n",
        "    return [vocab.get(word, 0) for word in re.findall(r'\\w+|\\S', sentence.lower())]\n",
        "\n",
        "# Função para decodificar uma sequência de índices em uma sentença de palavras\n",
        "def decode_sentence(encoded_sentence, vocab):\n",
        "    # Cria um dicionário inverso (index -> palavra) para poder mapear os índices de volta para palavras\n",
        "    reverse_vocab = {index: word for word, index in vocab.items()}\n",
        "\n",
        "    decoded_sentence = []\n",
        "    for index in encoded_sentence:\n",
        "        word = reverse_vocab.get(index, None)\n",
        "        if word is None:\n",
        "            # Se o índice não estiver no vocabulário, tenta encontrar a palavra mais próxima\n",
        "            unknown_word = '<UNK>'\n",
        "            closest_word = find_closest_word(unknown_word, vocab)\n",
        "            decoded_sentence.append(closest_word)\n",
        "        else:\n",
        "            decoded_sentence.append(word)\n",
        "\n",
        "    return ' '.join(decoded_sentence)\n",
        "\n",
        "# Exemplo de codificação e decodificação\n",
        "print('Encoder:')\n",
        "encoded = encode_sentence(text_lines_normalized[15], vocab)\n",
        "print(encoded)  # Exibe a versão codificada da 16ª linha de texto (index 15)\n",
        "print(text_lines_normalized[15])  # Exibe o texto original da linha 16\n",
        "\n",
        "print('Decoder:')\n",
        "decoded = decode_sentence(encoded, vocab)\n",
        "print(decoded)  # Exibe a versão decodificada da linha codificada com substituição\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtXlz9bQn0WX",
        "outputId": "6c743a50-b420-4584-cb32-b51561eed048"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder:\n",
            "[18, 10, 65, 22, 7, 0, 4, 10, 65, 6, 0, 7, 248, 129, 532, 4, 69, 0, 246, 4, 7, 11]\n",
            "com o outro para que subisse, o outro a teimar que queria ir morrer, tão tenazes ambos, que não\n",
            "Decoder:\n",
            "com o outro para que [UNK] , o outro a [UNK] que queria ir morrer , tão [UNK] ambos , que não\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividindo em conjuntos de treino e validação\n",
        "train_data, val_data = train_test_split(\n",
        "    text_lines_normalized,\n",
        "    test_size=0.2,\n",
        "    random_state=18)\n",
        "\n",
        "# Não utilize o split val para nada a partir daqui, somente validar"
      ],
      "metadata": {
        "id": "2ol9bz7bH4qk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wia_ygbvzJ_J"
      },
      "source": [
        "## Classe do dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define o tamanho do contexto: 9 palavras de entrada, a próxima palavra é o alvo (target)\n",
        "context_size = 9\n",
        "\n",
        "# Definição dos tokens especiais a partir do vocabulário\n",
        "SOS = vocab['[SOS]']\n",
        "EOS = vocab['[EOS]']\n",
        "PAD = vocab['[PAD]']\n",
        "UNK = vocab['[UNK]']\n",
        "\n",
        "# Criação do dataset personalizado para o texto de Machado\n",
        "class MachadoDataset(Dataset):\n",
        "    def __init__(self, text, context_size, vocab):\n",
        "        self.text = text  # Texto de entrada (linhas normalizadas)\n",
        "        self.context_size = context_size  # Tamanho do contexto (número de palavras de entrada)\n",
        "        self.vocab = vocab  # Vocabulário (mapeamento de palavras para índices)\n",
        "        self.input_target_pairs = self.prepare_dataset()  # Prepara as entradas e alvos\n",
        "\n",
        "    # Função para preparar o dataset com pares de entrada e alvo\n",
        "    def prepare_dataset(self):\n",
        "        input_target_pairs = []\n",
        "\n",
        "        # Itera sobre cada linha de texto\n",
        "        for item in self.text:\n",
        "            # Codifica a sentença e adiciona os tokens SOS e EOS\n",
        "            tokens = [SOS] + encode_sentence(item, self.vocab) + [EOS]\n",
        "\n",
        "            # Trunca os tokens para múltiplos do tamanho do contexto\n",
        "            max_len = ((len(tokens) - 1) // self.context_size) * self.context_size\n",
        "            tokens = tokens[:max_len]  # Limita o comprimento dos tokens\n",
        "            tokens.extend([EOS])  # Adiciona o token EOS ao final\n",
        "\n",
        "            # Desliza uma janela sobre os tokens para criar pares de entrada e alvo\n",
        "            for i in range(len(tokens) - self.context_size):\n",
        "                context = tokens[i:i + self.context_size]  # Contexto de entrada\n",
        "                target = tokens[i + 1:i + self.context_size + 1]  # Alvo (próximas palavras)\n",
        "\n",
        "                # Substitui tokens UNK por PAD para evitar palavras desconhecidas\n",
        "                target = [PAD if token == UNK else token for token in target]\n",
        "\n",
        "                # Adiciona o par de tensores de entrada e alvo à lista\n",
        "                input_target_pairs.append((torch.tensor(context), torch.tensor(target)))\n",
        "\n",
        "        return input_target_pairs\n",
        "\n",
        "    # Retorna o tamanho do dataset (número de pares de entrada e alvo)\n",
        "    def __len__(self):\n",
        "        return len(self.input_target_pairs)\n",
        "\n",
        "    # Retorna o par de entrada e alvo correspondente ao índice fornecido\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_target_pairs[idx]\n"
      ],
      "metadata": {
        "id": "Surq3CyCoSoR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criação dos datasets de treinamento e validação usando a classe MachadoDataset\n",
        "train_data = MachadoDataset(train_data, context_size, vocab)\n",
        "val_data = MachadoDataset(val_data, context_size, vocab)"
      ],
      "metadata": {
        "id": "H9uQL41FotYq"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define o tamanho do lote (batch) e cria os DataLoaders para treinamento e validação\n",
        "batch_size = 1000  # Define o tamanho de cada lote de dados (aumentado para 1000)\n",
        "\n",
        "# Cria o DataLoader para o conjunto de treinamento, com shuffle ativado para misturar os dados\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Cria o DataLoader para o conjunto de validação, também com shuffle ativado\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Pega um lote de exemplos do DataLoader de treinamento\n",
        "sample = next(iter(train_loader))\n",
        "\n",
        "# Exibe a forma (shape) do primeiro elemento do lote (entrada do modelo)\n",
        "sample[0].shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyji51_6pH55",
        "outputId": "62850202-f532-4b24-9705-81dc672f9199"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1000, 9])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5_-Yud0zJ_K"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi-Head Self-Attention Layer\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, model_dim, heads_count):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        # Verifica se a dimensão do modelo pode ser dividida igualmente pelos \"heads\"\n",
        "        assert model_dim % heads_count == 0\n",
        "\n",
        "        # Define o número de cabeças (heads) e a profundidade de cada cabeça\n",
        "        self.heads_count = heads_count\n",
        "        self.depth = model_dim // heads_count\n",
        "\n",
        "        # Camadas lineares para gerar queries, keys, e values\n",
        "        self.value_layer = nn.Linear(model_dim, model_dim, bias=False)\n",
        "        self.key_layer = nn.Linear(model_dim, model_dim, bias=False)\n",
        "        self.query_layer = nn.Linear(model_dim, model_dim, bias=False)\n",
        "        self.output_layer = nn.Linear(model_dim, model_dim, bias=False)\n",
        "\n",
        "    # Função para reformatar a entrada para dividir em várias cabeças\n",
        "    def reshape_heads(self, x):\n",
        "        batch_size, seq_len, model_dim = x.size()\n",
        "        x = x.view(batch_size, seq_len, self.heads_count, self.depth)\n",
        "        return x.transpose(1, 2)\n",
        "\n",
        "    # Função que calcula o produto escalar (dot-product) com escala para a atenção\n",
        "    def scaled_dot_product(self, Q, K, V):\n",
        "        score = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.depth)\n",
        "        attention_weights = torch.softmax(score, dim=-1)\n",
        "        output = torch.matmul(attention_weights, V)\n",
        "        return output, attention_weights\n",
        "\n",
        "    # Função para \"juntar\" as várias cabeças em uma única saída\n",
        "    def merge_heads(self, x):\n",
        "        batch_size, heads_count, seq_len, depth = x.size()\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, heads_count * depth)\n",
        "\n",
        "    # Função de passagem do modelo (forward pass)\n",
        "    def forward(self, queries, keys, values):\n",
        "        # Gera Q (queries), K (keys) e V (values)\n",
        "        Q = self.query_layer(queries)\n",
        "        K = self.key_layer(keys)\n",
        "        V = self.value_layer(values)\n",
        "\n",
        "        # Reorganiza para múltiplas cabeças\n",
        "        Q = self.reshape_heads(Q)\n",
        "        K = self.reshape_heads(K)\n",
        "        V = self.reshape_heads(V)\n",
        "\n",
        "        # Calcula a atenção e os pesos\n",
        "        attn_output, attn_weights = self.scaled_dot_product(Q, K, V)\n",
        "        merged = self.merge_heads(attn_output)\n",
        "        final_output = self.output_layer(merged)\n",
        "\n",
        "        return final_output, attn_weights\n",
        "\n",
        "# Camada de Decoder do Transformer\n",
        "class TransformerDecoderLayer(nn.Module):\n",
        "    def __init__(self, model_dim, heads_count, hidden_dim, dropout_rate=0.1):\n",
        "        super(TransformerDecoderLayer, self).__init__()\n",
        "        self.self_attention = MultiHeadSelfAttention(model_dim, heads_count)\n",
        "\n",
        "        # Normalização e camadas fully connected\n",
        "        self.layer_norm1 = nn.LayerNorm(model_dim)\n",
        "        self.layer_norm2 = nn.LayerNorm(model_dim)\n",
        "\n",
        "        self.fc1 = nn.Linear(model_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, model_dim)\n",
        "\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "\n",
        "    # Função de passagem do modelo\n",
        "    def forward(self, x):\n",
        "        attn_output, _ = self.self_attention(x, x, x)  # Autoatenção\n",
        "        x = x + self.dropout1(attn_output)  # Residual connection + dropout\n",
        "        x = self.layer_norm1(x)  # Normalização\n",
        "\n",
        "        ff_output = self.fc2(F.relu(self.fc1(x)))  # Rede feed-forward\n",
        "        x = x + self.dropout2(ff_output)  # Residual connection + dropout\n",
        "        x = self.layer_norm2(x)  # Normalização\n",
        "        return x\n",
        "\n",
        "# Codificação Posicional para adicionar informações de posição ao modelo\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    # Adiciona a codificação posicional aos embeddings\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return x\n",
        "\n",
        "# Modelo completo de linguagem baseado em Transformer\n",
        "class LanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, context_size, hidden_dim, num_heads, num_layers, dropout):\n",
        "        super(LanguageModel, self).__init__()\n",
        "\n",
        "        # Camadas de embedding e codificação posicional\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.positional_encoding = PositionalEncoding(embedding_dim, context_size)\n",
        "\n",
        "        # Múltiplas camadas de Transformer Decoder\n",
        "        self.decoder_layers = nn.ModuleList([\n",
        "            TransformerDecoderLayer(embedding_dim, num_heads, hidden_dim, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Camadas fully connected para prever o vocabulário\n",
        "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    # Função de passagem do modelo\n",
        "    def forward(self, inputs):\n",
        "        o = self.embeddings(inputs)\n",
        "        o = self.positional_encoding(o)\n",
        "\n",
        "        # Passa o embedding pelas camadas do decoder\n",
        "        for layer in self.decoder_layers:\n",
        "            o = layer(o)\n",
        "\n",
        "        # Rede fully connected para previsão final\n",
        "        o = F.relu(self.fc1(o))\n",
        "        o = F.relu(self.fc2(o))\n",
        "        o = self.fc3(o)\n",
        "\n",
        "        return o\n",
        "\n",
        "# Inicialização do modelo\n",
        "embedding_dim = 64  # Tamanho da representação das palavras (model_dim)\n",
        "vocab_size = 5000\n",
        "context_size = 9\n",
        "hidden_dim = embedding_dim * context_size\n",
        "num_layers = 2\n",
        "num_heads = 4\n",
        "dropout = 0.3\n",
        "\n",
        "# Criação do modelo de linguagem\n",
        "model = LanguageModel(vocab_size, embedding_dim, context_size, hidden_dim, num_heads, num_layers, dropout)\n"
      ],
      "metadata": {
        "id": "w3skouVdqcF5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pega o primeiro lote de exemplos do DataLoader de treinamento\n",
        "sample = next(iter(train_loader))\n",
        "\n",
        "# Separa o lote em entradas e alvos (targets)\n",
        "input = sample[0]  # Entradas do modelo (contexto)\n",
        "target = sample[1]  # Alvos do modelo (palavras subsequentes)\n"
      ],
      "metadata": {
        "id": "qgTZy1DuqjhA"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Passa o lote de entrada pelo modelo para obter a saída\n",
        "output = model(input)\n",
        "\n",
        "# Obtém o índice da palavra com a maior probabilidade para cada sequência de saída\n",
        "predicted = output.argmax(dim=1)\n",
        "\n",
        "target"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWHRyJmoqqJW",
        "outputId": "01ba7458-cc55-41df-93f9-31741a740ab8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2301,   12,    9,  ...,  725,   25,    6],\n",
              "        [  43,   10,   25,  ...,    6, 4326,    4],\n",
              "        [ 229,    4,    7,  ...,    3,   10,  600],\n",
              "        ...,\n",
              "        [ 522,   27,  176,  ...,    3,    4,    1],\n",
              "        [2290,   37,    3,  ..., 1412,    3,    1],\n",
              "        [   4,    7,   10,  ...,  202,    7,    1]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UngUhyu7zJ_L"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verifica se há uma GPU disponível e define o dispositivo de execução\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Move o modelo para o dispositivo (GPU se disponível, ou CPU caso contrário)\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1Bny4EcrNL2",
        "outputId": "c722fd23-6cd4-476b-bd0c-e3c019cf3537"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LanguageModel(\n",
              "  (embeddings): Embedding(5000, 64)\n",
              "  (positional_encoding): PositionalEncoding()\n",
              "  (decoder_layers): ModuleList(\n",
              "    (0-1): 2 x TransformerDecoderLayer(\n",
              "      (self_attention): MultiHeadSelfAttention(\n",
              "        (value_layer): Linear(in_features=64, out_features=64, bias=False)\n",
              "        (key_layer): Linear(in_features=64, out_features=64, bias=False)\n",
              "        (query_layer): Linear(in_features=64, out_features=64, bias=False)\n",
              "        (output_layer): Linear(in_features=64, out_features=64, bias=False)\n",
              "      )\n",
              "      (layer_norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (layer_norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (fc1): Linear(in_features=64, out_features=576, bias=True)\n",
              "      (fc2): Linear(in_features=576, out_features=64, bias=True)\n",
              "      (dropout1): Dropout(p=0.3, inplace=False)\n",
              "      (dropout2): Dropout(p=0.3, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (fc1): Linear(in_features=64, out_features=576, bias=True)\n",
              "  (fc2): Linear(in_features=576, out_features=576, bias=True)\n",
              "  (fc3): Linear(in_features=576, out_features=5000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definição de hiperparâmetros\n",
        "epochs = 10  # Número de épocas (quantas vezes o modelo verá o conjunto de dados completo)\n",
        "lr = 1e-4  # Taxa de aprendizado (learning rate)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD)  # Função de perda, ignorando tokens PAD\n",
        "optimizer = optim.AdamW(model.parameters(), lr=lr)  # Otimizador AdamW, ajustando os parâmetros do modelo\n",
        "\n",
        "# Inicializa a perda acumulada para o conjunto de validação\n",
        "val_running_loss = 0.0\n",
        "\n",
        "# Modo de avaliação (não há backpropagation)\n",
        "model.eval()\n",
        "with torch.no_grad():  # Desativa o cálculo de gradientes (economiza memória e acelera a avaliação)\n",
        "    for inputs, labels in val_loader:  # Itera sobre o conjunto de validação\n",
        "        # Move os dados (entradas e rótulos) para o dispositivo correto (GPU ou CPU)\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Passa os inputs pelo modelo para obter as previsões\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Ajusta a dimensão das saídas para calcular a perda\n",
        "        outputs = outputs.view(-1, vocab_size)  # Redimensiona para (batch_size * context_size, vocab_size)\n",
        "        labels = labels.view(-1)  # Redimensiona para (batch_size * context_size)\n",
        "\n",
        "        # Calcula a perda comparando as saídas preditas com os rótulos verdadeiros\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Acumula a perda ponderada pelo tamanho do lote\n",
        "        val_running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "# Calcula a perda média por exemplo no conjunto de validação\n",
        "initial_val_epoch_loss = val_running_loss / len(val_loader.dataset)\n",
        "\n",
        "# Calcula a perplexidade (exponeciação da perda média)\n",
        "initial_val_perplexity = np.exp(initial_val_epoch_loss)\n",
        "\n",
        "# Imprime a perda e a perplexidade inicial no conjunto de validação\n",
        "print(f'Initial Validation Loss: {initial_val_epoch_loss:.4f}, Initial Validation Perplexity: {initial_val_perplexity:.2f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELLzOYCPr_Hg",
        "outputId": "7cd4df10-819c-4c10-c4bf-ae20197088a5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Validation Loss: 8.5295, Initial Validation Perplexity: 5061.90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Listas para armazenar o loss e a acurácia por época\n",
        "train_loss_history = []\n",
        "val_loss_history = []\n",
        "train_accuracy_history = []\n",
        "val_accuracy_history = []\n",
        "\n",
        "# Loop de treinamento por época\n",
        "for epoch in range(epochs):\n",
        "    start_time = time.time()  # Marca o início da época\n",
        "    model.train()  # Coloca o modelo em modo de treinamento\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    # Loop de treinamento para cada batch\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)  # Mover dados para GPU/CPU\n",
        "        outputs = model(inputs)  # Passa os inputs pelo modelo\n",
        "\n",
        "        # Ajustar dimensões para a função de perda\n",
        "        outputs = outputs.view(-1, vocab_size)  # Formato para calcular a perda\n",
        "        labels = labels.view(-1)\n",
        "\n",
        "        # Calcular a perda\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()  # Zera os gradientes acumulados\n",
        "        loss.backward()  # Calcula os gradientes\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Prevenir gradientes explodindo\n",
        "        optimizer.step()  # Atualiza os parâmetros\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)  # Acumula a perda para calcular a média depois\n",
        "\n",
        "        # Calcular a acurácia\n",
        "        _, predicted = torch.max(outputs, 1)  # Obter a predição com maior probabilidade\n",
        "        correct_predictions += (predicted == labels).sum().item()  # Contar predições corretas\n",
        "        total_predictions += labels.size(0)  # Contar o total de predições\n",
        "\n",
        "    # Calcula a perda média e acurácia de treinamento por época\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    epoch_accuracy = correct_predictions / total_predictions\n",
        "\n",
        "    # Calcular a perplexidade (exponeciação da perda)\n",
        "    perplexity = np.exp(epoch_loss)\n",
        "\n",
        "    end_time = time.time()  # Tempo final da época\n",
        "    epoch_duration = end_time - start_time  # Duração da época\n",
        "\n",
        "    #### Validação\n",
        "    model.eval()  # Coloca o modelo em modo de validação\n",
        "    val_running_loss = 0.0\n",
        "    val_correct_predictions = 0\n",
        "    val_total_predictions = 0\n",
        "\n",
        "    # Desabilitar gradientes durante a validação para economizar memória\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Ajustar dimensões para a função de perda\n",
        "            outputs = outputs.view(-1, vocab_size)\n",
        "            labels = labels.view(-1)\n",
        "\n",
        "            # Calcular a perda\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            # Calcular a acurácia de validação\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            val_correct_predictions += (predicted == labels).sum().item()\n",
        "            val_total_predictions += labels.size(0)\n",
        "\n",
        "    # Calcula a perda e acurácia média de validação por época\n",
        "    val_epoch_loss = val_running_loss / len(val_loader.dataset)\n",
        "    val_epoch_accuracy = val_correct_predictions / val_total_predictions\n",
        "\n",
        "    # Calcular a perplexidade para o conjunto de validação\n",
        "    val_perplexity = np.exp(val_epoch_loss)\n",
        "\n",
        "    # Armazenar os valores de perda e acurácia para visualização posterior\n",
        "    train_loss_history.append(epoch_loss)\n",
        "    val_loss_history.append(val_epoch_loss)\n",
        "    train_accuracy_history.append(epoch_accuracy)\n",
        "    val_accuracy_history.append(val_epoch_accuracy)\n",
        "\n",
        "    # Exibir os resultados da época atual\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], '\n",
        "          f'Loss: {epoch_loss:.4f}, '\n",
        "          f'Accuracy: {epoch_accuracy:.4f}, '\n",
        "          f'Perplexity: {perplexity:.2f}, '\n",
        "          f'Elapsed Time: {epoch_duration:.2f} sec, '\n",
        "          f'Validation Loss: {val_epoch_loss:.4f}, '\n",
        "          f'Validation Accuracy: {val_epoch_accuracy:.4f}, '\n",
        "          f'Validation Perplexity: {val_perplexity:.2f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H62ofSgxsnVA",
        "outputId": "1a22abe7-14e2-4c3d-9628-f80c65468543"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 4.3508, Accuracy: 0.3156, Perplexity: 77.54, Elapsed Time: 58.13 sec, Validation Loss: 2.2009, Validation Accuracy: 0.5918, Validation Perplexity: 9.03\n",
            "Epoch [2/10], Loss: 1.6952, Accuracy: 0.6522, Perplexity: 5.45, Elapsed Time: 60.03 sec, Validation Loss: 0.9654, Validation Accuracy: 0.7622, Validation Perplexity: 2.63\n",
            "Epoch [3/10], Loss: 1.0215, Accuracy: 0.7486, Perplexity: 2.78, Elapsed Time: 62.26 sec, Validation Loss: 0.7267, Validation Accuracy: 0.7962, Validation Perplexity: 2.07\n",
            "Epoch [4/10], Loss: 0.7971, Accuracy: 0.7813, Perplexity: 2.22, Elapsed Time: 63.52 sec, Validation Loss: 0.6277, Validation Accuracy: 0.8090, Validation Perplexity: 1.87\n",
            "Epoch [5/10], Loss: 0.6872, Accuracy: 0.7966, Perplexity: 1.99, Elapsed Time: 63.74 sec, Validation Loss: 0.5787, Validation Accuracy: 0.8152, Validation Perplexity: 1.78\n",
            "Epoch [6/10], Loss: 0.6261, Accuracy: 0.8050, Perplexity: 1.87, Elapsed Time: 63.65 sec, Validation Loss: 0.5508, Validation Accuracy: 0.8183, Validation Perplexity: 1.73\n",
            "Epoch [7/10], Loss: 0.5877, Accuracy: 0.8100, Perplexity: 1.80, Elapsed Time: 63.67 sec, Validation Loss: 0.5324, Validation Accuracy: 0.8203, Validation Perplexity: 1.70\n",
            "Epoch [8/10], Loss: 0.5623, Accuracy: 0.8135, Perplexity: 1.75, Elapsed Time: 63.51 sec, Validation Loss: 0.5208, Validation Accuracy: 0.8216, Validation Perplexity: 1.68\n",
            "Epoch [9/10], Loss: 0.5451, Accuracy: 0.8157, Perplexity: 1.72, Elapsed Time: 63.59 sec, Validation Loss: 0.5120, Validation Accuracy: 0.8225, Validation Perplexity: 1.67\n",
            "Epoch [10/10], Loss: 0.5332, Accuracy: 0.8172, Perplexity: 1.70, Elapsed Time: 63.54 sec, Validation Loss: 0.5062, Validation Accuracy: 0.8231, Validation Perplexity: 1.66\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cria um subplot com dois gráficos lado a lado\n",
        "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Perda ao longo das Épocas\", \"Acurácia ao longo das Épocas\"))\n",
        "\n",
        "# Adiciona o gráfico de Perda (Loss)\n",
        "fig.add_trace(go.Scatter(x=list(range(len(train_loss_history))),\n",
        "                         y=train_loss_history,\n",
        "                         mode='lines',\n",
        "                         name='Perda no Treinamento'),\n",
        "              row=1, col=1)\n",
        "\n",
        "fig.add_trace(go.Scatter(x=list(range(len(val_loss_history))),\n",
        "                         y=val_loss_history,\n",
        "                         mode='lines',\n",
        "                         name='Perda na Validação'),\n",
        "              row=1, col=1)\n",
        "\n",
        "# Adiciona o gráfico de Acurácia\n",
        "fig.add_trace(go.Scatter(x=list(range(len(train_accuracy_history))),\n",
        "                         y=train_accuracy_history,\n",
        "                         mode='lines',\n",
        "                         name='Acurácia no Treinamento'),\n",
        "              row=1, col=2)\n",
        "\n",
        "fig.add_trace(go.Scatter(x=list(range(len(val_accuracy_history))),\n",
        "                         y=val_accuracy_history,\n",
        "                         mode='lines',\n",
        "                         name='Acurácia na Validação'),\n",
        "              row=1, col=2)\n",
        "\n",
        "# Atualiza os títulos e os eixos dos gráficos individualmente\n",
        "fig.update_xaxes(title_text=\"Épocas\", row=1, col=1)\n",
        "fig.update_xaxes(title_text=\"Épocas\", row=1, col=2)\n",
        "\n",
        "fig.update_yaxes(title_text=\"Perda\", row=1, col=1)\n",
        "fig.update_yaxes(title_text=\"Acurácia\", row=1, col=2)\n",
        "\n",
        "# Atualiza o layout geral do gráfico\n",
        "fig.update_layout(height=600, width=1200,\n",
        "                  title_text=\"Perda e Acurácia ao longo das Épocas\")\n",
        "\n",
        "# Exibe o gráfico\n",
        "fig.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "ctxCUlkRwhi4",
        "outputId": "1373afef-b48c-4b06-f178-6b47397eccb3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"3cc1b123-a3c4-4d3c-9bfa-1e89c9f313bf\" class=\"plotly-graph-div\" style=\"height:600px; width:1200px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"3cc1b123-a3c4-4d3c-9bfa-1e89c9f313bf\")) {                    Plotly.newPlot(                        \"3cc1b123-a3c4-4d3c-9bfa-1e89c9f313bf\",                        [{\"mode\":\"lines\",\"name\":\"Perda no Treinamento\",\"x\":[0,1,2,3,4,5,6,7,8,9],\"y\":[4.350837515098261,1.6951701656692024,1.0214530973428062,0.7970764037092635,0.6871821634648788,0.6261384487717873,0.587689516778592,0.5622950844080812,0.5451261104216578,0.5331556421132337],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"mode\":\"lines\",\"name\":\"Perda na Valida\\u00e7\\u00e3o\",\"x\":[0,1,2,3,4,5,6,7,8,9],\"y\":[2.20093569970021,0.9654434307169274,0.7267151333415909,0.6277376678822776,0.5787251610632121,0.5508062747648539,0.5324395982671828,0.5208197365062405,0.5119849731095152,0.5062342082734125],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"mode\":\"lines\",\"name\":\"Acur\\u00e1cia no Treinamento\",\"x\":[0,1,2,3,4,5,6,7,8,9],\"y\":[0.31561535665486745,0.652171103370539,0.748635394566721,0.7812711582702175,0.7965629081574519,0.8050026743582718,0.8100466411849291,0.8134938795804271,0.8156792728758863,0.8172390405457197],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"mode\":\"lines\",\"name\":\"Acur\\u00e1cia na Valida\\u00e7\\u00e3o\",\"x\":[0,1,2,3,4,5,6,7,8,9],\"y\":[0.5917662765124126,0.7622412780187997,0.7962084237165582,0.8089571658833454,0.8152246776331646,0.818299119516751,0.8203054388105567,0.8215712747047481,0.8225038224572186,0.8230866059592673],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,0.45],\"title\":{\"text\":\"\\u00c9pocas\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Perda\"}},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.55,1.0],\"title\":{\"text\":\"\\u00c9pocas\"}},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Acur\\u00e1cia\"}},\"annotations\":[{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Perda ao longo das \\u00c9pocas\",\"x\":0.225,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Acur\\u00e1cia ao longo das \\u00c9pocas\",\"x\":0.775,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"title\":{\"text\":\"Perda e Acur\\u00e1cia ao longo das \\u00c9pocas\"},\"height\":600,\"width\":1200},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('3cc1b123-a3c4-4d3c-9bfa-1e89c9f313bf');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1zhxVqfzJ_M"
      },
      "source": [
        "## Exemplo de uso"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, vocab, text, max_length, context):\n",
        "    \"\"\"Gere texto até atingir max_length usando o modelo e o vocabulário fornecidos.\"\"\"\n",
        "    model.eval()\n",
        "    encoded_text = encode_sentence(text, vocab)\n",
        "    encoded_text_tensor = torch.tensor(encoded_text, device=device).unsqueeze(0)\n",
        "    generated_text = list(encoded_text)\n",
        "\n",
        "    while len(generated_text) < max_length:\n",
        "        # Obtenha o contexto para a previsão\n",
        "        if len(generated_text) >= context:\n",
        "            context_input = generated_text[-context:]\n",
        "        else:\n",
        "            context_input = [PAD] * (context - len(generated_text)) + generated_text\n",
        "        context_input_tensor = torch.tensor(context_input, device=device).unsqueeze(0)\n",
        "\n",
        "        # Gere a previsão para o próximo token\n",
        "        with torch.no_grad():\n",
        "            logits = model(context_input_tensor)  # Saída do modelo: [1, context, vocab_size]\n",
        "            logits = logits[:, -1, :]  # Pegue a previsão para o último token do contexto\n",
        "            probabilities = F.softmax(logits, dim=-1)  # Calcule as probabilidades\n",
        "            next_token = torch.multinomial(probabilities[0], num_samples=1).item()\n",
        "\n",
        "        # Se atingir o token EOS, pare a geração\n",
        "        if next_token == EOS:\n",
        "            break\n",
        "        generated_text.append(next_token)\n",
        "\n",
        "    # Decodifique o texto gerado\n",
        "    generated_text_decoded = decode_sentence(generated_text, vocab)\n",
        "    return generated_text_decoded\n",
        "\n",
        "# Exemplos de uso\n",
        "t1 = \"uma triste, crua e eram nove horas da noite\"\n",
        "context = 9\n",
        "max_length = 50\n",
        "\n",
        "text2 = \"luís alves sacudiu a cabeça e sorriu\"\n",
        "context = 9\n",
        "max_length = 50\n",
        "\n",
        "text3 = \"o golpe foi profundo\"\n",
        "context = 9\n",
        "max_length = 50\n",
        "\n",
        "text4 = \"uma bagatela de vinte anos\"\n",
        "context = 9\n",
        "max_length = 50\n",
        "\n",
        "text5 = \"use morre por tão pouco morrese\"\n",
        "context = 9\n",
        "max_length = 50\n",
        "\n",
        "# Gerar e imprimir 3 frases de forma randômica do t1\n",
        "print(\"Texto 1\")\n",
        "for i in range(3):\n",
        "    generated_text = generate_text(model, vocab, t1, max_length, context)\n",
        "    print(f\"Frase {i+1}: {generated_text}\\n\")\n",
        "\n",
        "# Gerar e imprimir 3 frases de forma randômica do t2\n",
        "print(\"Texto 2\")\n",
        "for i in range(3):\n",
        "    generated_text = generate_text(model, vocab, text2, max_length, context)\n",
        "    print(f\"Frase {i+1}: {generated_text}\\n\")\n",
        "\n",
        "# Gerar e imprimir 3 frases de forma randômica do t3\n",
        "print(\"Texto 3\")\n",
        "for i in range(3):\n",
        "    generated_text = generate_text(model, vocab, text3, max_length, context)\n",
        "    print(f\"Frase {i+1}: {generated_text}\\n\")\n",
        "\n",
        "# Gerar e imprimir 3 frases de forma randômica do t4\n",
        "print(\"Texto 4\")\n",
        "for i in range(3):\n",
        "    generated_text = generate_text(model, vocab, text4, max_length, context)\n",
        "    print(f\"Frase {i+1}: {generated_text}\\n\")\n",
        "\n",
        "    # Gerar e imprimir 3 frases de forma randômica do t5\n",
        "print(\"Texto 5\")\n",
        "for i in range(3):\n",
        "    generated_text = generate_text(model, vocab, text5, max_length, context)\n",
        "    print(f\"Frase {i+1}: {generated_text}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7WXW-nriy7A",
        "outputId": "e94b3b5d-3844-4253-b157-fb87377b8aac"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto 1\n",
            "Frase 1: uma triste , [UNK] e eram nove horas da noite , não tinha de causa da terra , onde mal , e\n",
            "\n",
            "Frase 2: uma triste , [UNK] e eram nove horas da noite , a\n",
            "\n",
            "Frase 3: uma triste , [UNK] e eram nove horas da noite filho e que ela a\n",
            "\n",
            "Texto 2\n",
            "Frase 1: luís alves sacudiu a cabeça e sorriu é a partida , e o pai , ou\n",
            "\n",
            "Frase 2: luís alves sacudiu a cabeça e sorriu faria , como a cabeça , a renan . entrou na vocação respondeu alma\n",
            "\n",
            "Frase 3: luís alves sacudiu a cabeça e sorriu , ou risco e muito uma idéia o diabo . não admira , nem\n",
            "\n",
            "Texto 3\n",
            "Frase 1: o golpe foi profundo ; ouvi nós o acaso a infeliz já na arte . ao pé . o cunhado , e para\n",
            "\n",
            "Frase 2: o golpe foi profundo . mas há\n",
            "\n",
            "Frase 3: o golpe foi profundo na sege . ele exige ciúmes . creio tudo\n",
            "\n",
            "Texto 4\n",
            "Frase 1: uma [UNK] de vinte anos , ergueuse cinco em si para crer maria ver um .\n",
            "\n",
            "Frase 2: uma [UNK] de vinte anos\n",
            "\n",
            "Frase 3: uma [UNK] de vinte anos , empregado não se sucedeu , de deixar regras . eu fazia que vinha da morte ; me deu ouvilo a minha\n",
            "\n",
            "Texto 5\n",
            "Frase 1: [UNK] morre por tão pouco [UNK] no\n",
            "\n",
            "Frase 2: [UNK] morre por tão pouco [UNK]\n",
            "\n",
            "Frase 3: [UNK] morre por tão pouco [UNK] out a de como bom ela ser a solteira , com bom os convivas parecia acompanhar da vida tem houvesse\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Refinamento com LoRA"
      ],
      "metadata": {
        "id": "xm_nQZw5FvUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definição da camada LoRA\n",
        "class LoRALayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Define uma camada LoRA que consiste em duas matrizes A e B.\n",
        "\n",
        "    Args:\n",
        "        in_dim: Dimensão de entrada (número de características ou características de entrada)\n",
        "        out_dim: Dimensão de saída\n",
        "        rank: Dimensão de redução (ou posto) da camada LoRA, que reduz a dimensionalidade\n",
        "        alpha: Fator de escala para ajustar a contribuição da LoRA ao modelo principal\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim, out_dim, rank, alpha=1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Define o desvio padrão usado na inicialização da matriz A.\n",
        "        # O desvio padrão é calculado como o inverso da raiz quadrada do rank.\n",
        "        std_dev = 1 / torch.sqrt(torch.tensor(rank).float())\n",
        "\n",
        "        # Inicializa a matriz A com valores aleatórios normalizados pelo desvio padrão calculado.\n",
        "        # A tem dimensão (in_dim, rank), o que significa que estamos reduzindo a dimensionalidade da entrada.\n",
        "        self.A = nn.Parameter(torch.randn(in_dim, rank) * std_dev)\n",
        "\n",
        "        # Inicializa a matriz B com zeros. B tem dimensão (rank, out_dim),\n",
        "        # que expande de volta para a dimensão de saída desejada.\n",
        "        self.B = nn.Parameter(torch.zeros(rank, out_dim))\n",
        "\n",
        "        # O fator de escala alpha ajusta a magnitude da transformação LoRA.\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def forward(self, x):\n",
        "        # No método forward, a entrada x é multiplicada pela matriz A (redução da dimensionalidade),\n",
        "        # seguida da multiplicação pela matriz B (expansão da dimensionalidade de volta ao tamanho original).\n",
        "        # O resultado é escalado por alpha e retornado como a saída da camada LoRA.\n",
        "        return self.alpha * (x @ self.A @ self.B)\n",
        "\n",
        "\n",
        "# Definição de uma camada linear combinada com LoRA\n",
        "class LinearWithLoRA(nn.Module):\n",
        "    \"\"\"\n",
        "    Combina uma camada linear tradicional com uma camada LoRA.\n",
        "\n",
        "    Args:\n",
        "        linear: Uma camada linear do PyTorch, já existente\n",
        "        rank: Dimensão de redução (ou posto) da LoRA\n",
        "        alpha: Fator de escala para ajustar a contribuição da LoRA\n",
        "    \"\"\"\n",
        "    def __init__(self, linear, rank, alpha=1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Armazena a camada linear que será usada em conjunto com LoRA.\n",
        "        self.linear = linear\n",
        "\n",
        "        # Cria a camada LoRA correspondente com a mesma dimensão de entrada e saída da camada linear.\n",
        "        # A LoRA será aplicada paralelamente à camada linear para ajustar o aprendizado.\n",
        "        self.lora = LoRALayer(\n",
        "            linear.in_features, linear.out_features, rank, alpha\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # O método forward aplica a camada linear tradicional e a camada LoRA separadamente.\n",
        "        # A saída final é a soma das duas saídas, combinando a transformação linear padrão com o ajuste LoRA.\n",
        "        return self.linear(x) + self.lora(x)\n",
        "\n",
        "\n",
        "# Definição de uma camada de embedding combinada com LoRA\n",
        "class EmbeddingWithLoRA(nn.Module):\n",
        "    \"\"\"\n",
        "    Combina uma camada de embeddings com uma camada LoRA.\n",
        "\n",
        "    Args:\n",
        "        embed: Uma camada de embeddings do PyTorch\n",
        "        rank: Dimensão de redução (ou posto) da LoRA\n",
        "        alpha: Fator de escala para ajustar a contribuição da LoRA\n",
        "    \"\"\"\n",
        "    def __init__(self, embed, rank, alpha=1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Armazena a camada de embedding original.\n",
        "        self.embedding = embed\n",
        "\n",
        "        # Cria uma camada LoRA que atua na dimensão dos embeddings.\n",
        "        # Tanto a dimensão de entrada quanto a de saída da LoRA são iguais à dimensão dos embeddings.\n",
        "        self.lora = LoRALayer(\n",
        "            embed.embedding_dim, embed.embedding_dim, rank, alpha\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # O método forward primeiro obtém os embeddings correspondentes para as entradas x.\n",
        "        embed = self.embedding(x)\n",
        "\n",
        "        # Em seguida, aplica a transformação LoRA sobre os embeddings.\n",
        "        lora_output = self.lora(embed)\n",
        "\n",
        "        # A saída final é a soma dos embeddings originais com o ajuste produzido pela LoRA.\n",
        "        return embed + lora_output\n"
      ],
      "metadata": {
        "id": "9tURP9vWH3FB"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspecionar a estrutura do modelo\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAA3NkQRbvEd",
        "outputId": "93e623dd-044f-4f5f-8766-fcb123c7aafc"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LanguageModel(\n",
            "  (embeddings): Embedding(5000, 64)\n",
            "  (positional_encoding): PositionalEncoding()\n",
            "  (decoder_layers): ModuleList(\n",
            "    (0-1): 2 x TransformerDecoderLayer(\n",
            "      (self_attention): MultiHeadSelfAttention(\n",
            "        (value_layer): Linear(in_features=64, out_features=64, bias=False)\n",
            "        (key_layer): Linear(in_features=64, out_features=64, bias=False)\n",
            "        (query_layer): Linear(in_features=64, out_features=64, bias=False)\n",
            "        (output_layer): Linear(in_features=64, out_features=64, bias=False)\n",
            "      )\n",
            "      (layer_norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      (layer_norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      (fc1): Linear(in_features=64, out_features=576, bias=True)\n",
            "      (fc2): Linear(in_features=576, out_features=64, bias=True)\n",
            "      (dropout1): Dropout(p=0.3, inplace=False)\n",
            "      (dropout2): Dropout(p=0.3, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (fc1): Linear(in_features=64, out_features=576, bias=True)\n",
            "  (fc2): Linear(in_features=576, out_features=576, bias=True)\n",
            "  (fc3): Linear(in_features=576, out_features=5000, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicar LoRA ao modelo\n",
        "import copy\n",
        "\n",
        "# Definição dos hiperparâmetros do LoRA\n",
        "lora_rank = 16\n",
        "lora_alpha = 2\n",
        "\n",
        "model_lora = copy.deepcopy(model)\n",
        "\n",
        "# Congelar todos os parâmetros do modelo\n",
        "for param in model_lora.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Aplicar modificações LoRA às embeddings\n",
        "model_lora.embeddings = EmbeddingWithLoRA(model_lora.embeddings, rank=lora_rank, alpha=lora_alpha)\n",
        "\n",
        "# Aplicar LoRA às camadas do transformador do modelo usando 'query_layer', 'key_layer', 'value_layer' e 'output_layer'\n",
        "for layer in model_lora.decoder_layers:\n",
        "    layer.self_attention.query_layer = LinearWithLoRA(layer.self_attention.query_layer, rank=lora_rank, alpha=lora_alpha)\n",
        "    layer.self_attention.key_layer = LinearWithLoRA(layer.self_attention.key_layer, rank=lora_rank, alpha=lora_alpha)\n",
        "    layer.self_attention.value_layer = LinearWithLoRA(layer.self_attention.value_layer, rank=lora_rank, alpha=lora_alpha)\n",
        "    layer.self_attention.output_layer = LinearWithLoRA(layer.self_attention.output_layer, rank=lora_rank, alpha=lora_alpha)\n",
        "\n",
        "    # Aplicar LoRA às camadas feedforward\n",
        "    layer.fc1 = LinearWithLoRA(layer.fc1, rank=lora_rank, alpha=lora_alpha)\n",
        "    layer.fc2 = LinearWithLoRA(layer.fc2, rank=lora_rank, alpha=lora_alpha)\n",
        "\n",
        "model_lora.fc1 = LinearWithLoRA(model_lora.fc1, rank=lora_rank, alpha=lora_alpha)\n",
        "model_lora.fc2 = LinearWithLoRA(model_lora.fc2, rank=lora_rank, alpha=lora_alpha)\n",
        "\n",
        "# Inspecionar a estrutura do modelo\n",
        "print(model_lora)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "id72m-dKbi0B",
        "outputId": "6c34543f-b879-4387-b23d-ddf71ad2fae4"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LanguageModel(\n",
            "  (embeddings): EmbeddingWithLoRA(\n",
            "    (embedding): Embedding(5000, 64)\n",
            "    (lora): LoRALayer()\n",
            "  )\n",
            "  (positional_encoding): PositionalEncoding()\n",
            "  (decoder_layers): ModuleList(\n",
            "    (0-1): 2 x TransformerDecoderLayer(\n",
            "      (self_attention): MultiHeadSelfAttention(\n",
            "        (value_layer): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=64, out_features=64, bias=False)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (key_layer): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=64, out_features=64, bias=False)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (query_layer): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=64, out_features=64, bias=False)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (output_layer): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=64, out_features=64, bias=False)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      (layer_norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      (fc1): LinearWithLoRA(\n",
            "        (linear): Linear(in_features=64, out_features=576, bias=True)\n",
            "        (lora): LoRALayer()\n",
            "      )\n",
            "      (fc2): LinearWithLoRA(\n",
            "        (linear): Linear(in_features=576, out_features=64, bias=True)\n",
            "        (lora): LoRALayer()\n",
            "      )\n",
            "      (dropout1): Dropout(p=0.3, inplace=False)\n",
            "      (dropout2): Dropout(p=0.3, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (fc1): LinearWithLoRA(\n",
            "    (linear): Linear(in_features=64, out_features=576, bias=True)\n",
            "    (lora): LoRALayer()\n",
            "  )\n",
            "  (fc2): LinearWithLoRA(\n",
            "    (linear): Linear(in_features=576, out_features=576, bias=True)\n",
            "    (lora): LoRALayer()\n",
            "  )\n",
            "  (fc3): Linear(in_features=576, out_features=5000, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para calcular a acurácia\n",
        "def calculate_accuracy(outputs, labels):\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    correct_predictions = (predicted == labels).sum().item()\n",
        "    return correct_predictions / labels.size(0)\n",
        "\n",
        "# Função para calcular o loss e a acurácia inicial\n",
        "def initial_evaluation(model, dataloader, criterion, device, vocab_size):\n",
        "    model.eval()  # Coloca o modelo em modo de avaliação\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    with torch.no_grad():  # Desabilita o cálculo de gradientes\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Obter as saídas do modelo LoRA\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Ajustar dimensões para a função de perda\n",
        "            outputs = outputs.view(-1, vocab_size)\n",
        "            labels = labels.view(-1)\n",
        "\n",
        "            # Calcular o loss\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            # Calcular a acurácia\n",
        "            correct_predictions += (torch.argmax(outputs, dim=1) == labels).sum().item()\n",
        "            total_predictions += labels.size(0)\n",
        "\n",
        "    # Perda e acurácia médias\n",
        "    avg_loss = running_loss / len(dataloader.dataset)\n",
        "    avg_accuracy = correct_predictions / total_predictions\n",
        "    perplexity = np.exp(avg_loss)\n",
        "\n",
        "    return avg_loss, avg_accuracy, perplexity\n",
        "\n",
        "# Avaliação inicial no conjunto de treinamento e validação\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_lora.to(device)  # Envia o modelo LoRA para o dispositivo\n",
        "\n",
        "# Avaliar no conjunto de treinamento\n",
        "train_loss, train_accuracy, train_perplexity = initial_evaluation(model_lora, train_loader, criterion, device, vocab_size)\n",
        "print(f'Initial Training Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.4f}, Perplexity: {train_perplexity:.2f}')\n",
        "\n",
        "# Avaliar no conjunto de validação\n",
        "val_loss, val_accuracy, val_perplexity = initial_evaluation(model_lora, val_loader, criterion, device, vocab_size)\n",
        "print(f'Initial Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}, Perplexity: {val_perplexity:.2f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kY9Hf07E3o0K",
        "outputId": "d93d12da-1fdf-4006-f513-cd7129dc3c4b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Training Loss: 0.4989, Accuracy: 0.8224, Perplexity: 1.65\n",
            "Initial Validation Loss: 0.5062, Accuracy: 0.8231, Perplexity: 1.66\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Função de treinamento e avaliação\n",
        "def train(model, train_loader, val_loader, criterion, optimizer, epochs, device):\n",
        "    # Listas para armazenar histórico de loss e acurácia\n",
        "    train_loss_history = []\n",
        "    val_loss_history = []\n",
        "    train_accuracy_history = []\n",
        "    val_accuracy_history = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Treinamento\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct_predictions = 0\n",
        "        total_predictions = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Zerar gradientes acumulados\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            outputs = outputs.view(-1, vocab_size)\n",
        "            labels = labels.view(-1)\n",
        "\n",
        "            # Cálculo do loss\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()  # Backpropagation\n",
        "            optimizer.step()  # Atualização dos pesos\n",
        "\n",
        "            # Acumular loss e calcular acurácia\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_predictions += (predicted == labels).sum().item()\n",
        "            total_predictions += labels.size(0)\n",
        "\n",
        "        # Loss e acurácia médios para o treinamento\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_accuracy = correct_predictions / total_predictions\n",
        "        train_loss_history.append(epoch_loss)\n",
        "        train_accuracy_history.append(epoch_accuracy)\n",
        "\n",
        "        # Validação\n",
        "        model.eval()\n",
        "        val_running_loss = 0.0\n",
        "        val_correct_predictions = 0\n",
        "        val_total_predictions = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                outputs = outputs.view(-1, vocab_size)\n",
        "                labels = labels.view(-1)\n",
        "\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_running_loss += loss.item() * inputs.size(0)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                val_correct_predictions += (predicted == labels).sum().item()\n",
        "                val_total_predictions += labels.size(0)\n",
        "\n",
        "        # Loss e acurácia médias para a validação\n",
        "        val_epoch_loss = val_running_loss / len(val_loader.dataset)\n",
        "        val_epoch_accuracy = val_correct_predictions / val_total_predictions\n",
        "        val_loss_history.append(val_epoch_loss)\n",
        "        val_accuracy_history.append(val_epoch_accuracy)\n",
        "\n",
        "        # Exibir resultados da época\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], '\n",
        "              f'Train Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_accuracy:.4f}, '\n",
        "              f'Validation Loss: {val_epoch_loss:.4f}, Validation Accuracy: {val_epoch_accuracy:.4f}')\n",
        "\n",
        "    # Retorna o histórico de treinamento para análise posterior\n",
        "    return {\n",
        "        'train_loss': train_loss_history,\n",
        "        'val_loss': val_loss_history,\n",
        "        'train_accuracy': train_accuracy_history,\n",
        "        'val_accuracy': val_accuracy_history\n",
        "    }\n",
        "\n",
        "# Definir os parâmetros do modelo e critério de perda\n",
        "epochs = 10\n",
        "lr = 1e-4\n",
        "PAD_TOKEN = vocab['[PAD]']  # Corrigir a chamada para usar o vocabulário\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
        "\n",
        "# Definir o otimizador AdamW para o modelo LoRA\n",
        "optimizer = torch.optim.AdamW(model_lora.parameters(), lr=lr)\n",
        "\n",
        "# Treinar o modelo LoRA\n",
        "history = train(model_lora, train_loader, val_loader, criterion, optimizer, epochs, device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_kja8uH6OlC",
        "outputId": "984568bf-cb46-4be9-af22-c8573faa3abb"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Train Loss: 0.5221, Train Accuracy: 0.8186, Validation Loss: 0.5027, Validation Accuracy: 0.8235\n",
            "Epoch [2/10], Train Loss: 0.5165, Train Accuracy: 0.8195, Validation Loss: 0.5005, Validation Accuracy: 0.8237\n",
            "Epoch [3/10], Train Loss: 0.5126, Train Accuracy: 0.8200, Validation Loss: 0.4990, Validation Accuracy: 0.8238\n",
            "Epoch [4/10], Train Loss: 0.5100, Train Accuracy: 0.8204, Validation Loss: 0.4975, Validation Accuracy: 0.8240\n",
            "Epoch [5/10], Train Loss: 0.5076, Train Accuracy: 0.8207, Validation Loss: 0.4966, Validation Accuracy: 0.8241\n",
            "Epoch [6/10], Train Loss: 0.5060, Train Accuracy: 0.8209, Validation Loss: 0.4958, Validation Accuracy: 0.8241\n",
            "Epoch [7/10], Train Loss: 0.5049, Train Accuracy: 0.8210, Validation Loss: 0.4953, Validation Accuracy: 0.8242\n",
            "Epoch [8/10], Train Loss: 0.5038, Train Accuracy: 0.8211, Validation Loss: 0.4952, Validation Accuracy: 0.8242\n",
            "Epoch [9/10], Train Loss: 0.5028, Train Accuracy: 0.8212, Validation Loss: 0.4944, Validation Accuracy: 0.8242\n",
            "Epoch [10/10], Train Loss: 0.5020, Train Accuracy: 0.8213, Validation Loss: 0.4940, Validation Accuracy: 0.8243\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cria um subplot com dois gráficos lado a lado\n",
        "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Perda ao longo das Épocas\", \"Acurácia ao longo das Épocas\"))\n",
        "\n",
        "# Adiciona o gráfico de Perda (Loss)\n",
        "fig.add_trace(go.Scatter(x=list(range(len(train_loss_history))),\n",
        "                         y=train_loss_history,\n",
        "                         mode='lines',\n",
        "                         name='Perda no Treinamento'),\n",
        "              row=1, col=1)\n",
        "\n",
        "fig.add_trace(go.Scatter(x=list(range(len(val_loss_history))),\n",
        "                         y=val_loss_history,\n",
        "                         mode='lines',\n",
        "                         name='Perda na Validação'),\n",
        "              row=1, col=1)\n",
        "\n",
        "# Adiciona o gráfico de Acurácia\n",
        "fig.add_trace(go.Scatter(x=list(range(len(train_accuracy_history))),\n",
        "                         y=train_accuracy_history,\n",
        "                         mode='lines',\n",
        "                         name='Acurácia no Treinamento'),\n",
        "              row=1, col=2)\n",
        "\n",
        "fig.add_trace(go.Scatter(x=list(range(len(val_accuracy_history))),\n",
        "                         y=val_accuracy_history,\n",
        "                         mode='lines',\n",
        "                         name='Acurácia na Validação'),\n",
        "              row=1, col=2)\n",
        "\n",
        "# Atualiza os títulos e os eixos dos gráficos individualmente\n",
        "fig.update_xaxes(title_text=\"Épocas\", row=1, col=1)\n",
        "fig.update_xaxes(title_text=\"Épocas\", row=1, col=2)\n",
        "\n",
        "fig.update_yaxes(title_text=\"Perda\", row=1, col=1)\n",
        "fig.update_yaxes(title_text=\"Acurácia\", row=1, col=2)\n",
        "\n",
        "# Atualiza o layout geral do gráfico\n",
        "fig.update_layout(height=600, width=1200, title_text=\"Perda e Acurácia ao longo das Épocas\")\n",
        "\n",
        "# Exibe o gráfico\n",
        "fig.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "x6qYAlJ9-Aop",
        "outputId": "3434207d-716c-4ece-944b-698608972c2c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"b5737169-9b3a-4f6a-a8dd-e87331f74a9a\" class=\"plotly-graph-div\" style=\"height:600px; width:1200px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"b5737169-9b3a-4f6a-a8dd-e87331f74a9a\")) {                    Plotly.newPlot(                        \"b5737169-9b3a-4f6a-a8dd-e87331f74a9a\",                        [{\"mode\":\"lines\",\"name\":\"Perda no Treinamento\",\"x\":[0,1,2,3,4,5,6,7,8,9],\"y\":[4.350837515098261,1.6951701656692024,1.0214530973428062,0.7970764037092635,0.6871821634648788,0.6261384487717873,0.587689516778592,0.5622950844080812,0.5451261104216578,0.5331556421132337],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"mode\":\"lines\",\"name\":\"Perda na Valida\\u00e7\\u00e3o\",\"x\":[0,1,2,3,4,5,6,7,8,9],\"y\":[2.20093569970021,0.9654434307169274,0.7267151333415909,0.6277376678822776,0.5787251610632121,0.5508062747648539,0.5324395982671828,0.5208197365062405,0.5119849731095152,0.5062342082734125],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"mode\":\"lines\",\"name\":\"Acur\\u00e1cia no Treinamento\",\"x\":[0,1,2,3,4,5,6,7,8,9],\"y\":[0.31561535665486745,0.652171103370539,0.748635394566721,0.7812711582702175,0.7965629081574519,0.8050026743582718,0.8100466411849291,0.8134938795804271,0.8156792728758863,0.8172390405457197],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"mode\":\"lines\",\"name\":\"Acur\\u00e1cia na Valida\\u00e7\\u00e3o\",\"x\":[0,1,2,3,4,5,6,7,8,9],\"y\":[0.5917662765124126,0.7622412780187997,0.7962084237165582,0.8089571658833454,0.8152246776331646,0.818299119516751,0.8203054388105567,0.8215712747047481,0.8225038224572186,0.8230866059592673],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,0.45],\"title\":{\"text\":\"\\u00c9pocas\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Perda\"}},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.55,1.0],\"title\":{\"text\":\"\\u00c9pocas\"}},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Acur\\u00e1cia\"}},\"annotations\":[{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Perda ao longo das \\u00c9pocas\",\"x\":0.225,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Acur\\u00e1cia ao longo das \\u00c9pocas\",\"x\":0.775,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"title\":{\"text\":\"Perda e Acur\\u00e1cia ao longo das \\u00c9pocas\"},\"height\":600,\"width\":1200},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('b5737169-9b3a-4f6a-a8dd-e87331f74a9a');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, vocab, text, max_length, context):\n",
        "    \"\"\"Gere texto até atingir max_length usando o modelo e o vocabulário fornecidos.\"\"\"\n",
        "    model.eval()\n",
        "    encoded_text = encode_sentence(text, vocab)\n",
        "    encoded_text_tensor = torch.tensor(encoded_text, device=device).unsqueeze(0)\n",
        "    generated_text = list(encoded_text)\n",
        "\n",
        "    while len(generated_text) < max_length:\n",
        "        # Obtenha o contexto para a previsão\n",
        "        if len(generated_text) >= context:\n",
        "            context_input = generated_text[-context:]\n",
        "        else:\n",
        "            context_input = [PAD] * (context - len(generated_text)) + generated_text\n",
        "        context_input_tensor = torch.tensor(context_input, device=device).unsqueeze(0)\n",
        "\n",
        "        # Gere a previsão para o próximo token\n",
        "        with torch.no_grad():\n",
        "            logits = model(context_input_tensor)  # Saída do modelo: [1, context, vocab_size]\n",
        "            logits = logits[:, -1, :]  # Pegue a previsão para o último token do contexto\n",
        "            probabilities = F.softmax(logits, dim=-1)  # Calcule as probabilidades\n",
        "            next_token = torch.multinomial(probabilities[0], num_samples=1).item()\n",
        "\n",
        "        # Se atingir o token EOS, pare a geração\n",
        "        if next_token == EOS:\n",
        "            break\n",
        "        generated_text.append(next_token)\n",
        "\n",
        "    # Decodifique o texto gerado\n",
        "    generated_text_decoded = decode_sentence(generated_text, vocab)\n",
        "    return generated_text_decoded\n",
        "\n",
        "# Exemplos de uso\n",
        "t1 = \"uma triste, crua e eram nove horas da noite\"\n",
        "context = 9\n",
        "max_length = 50\n",
        "\n",
        "text2 = \"luís alves sacudiu a cabeça e sorriu\"\n",
        "context = 9\n",
        "max_length = 50\n",
        "\n",
        "text3 = \"o golpe foi profundo\"\n",
        "context = 9\n",
        "max_length = 50\n",
        "\n",
        "text4 = \"uma bagatela de vinte anos\"\n",
        "context = 9\n",
        "max_length = 50\n",
        "\n",
        "text5 = \"use morre por tão pouco morrese\"\n",
        "context = 9\n",
        "max_length = 50\n",
        "\n",
        "# Gerar e imprimir 3 frases de forma randômica do t1\n",
        "print(\"Texto 1\")\n",
        "for i in range(3):\n",
        "    generated_text = generate_text(model, vocab, t1, max_length, context)\n",
        "    print(f\"Frase {i+1}: {generated_text}\\n\")\n",
        "\n",
        "# Gerar e imprimir 3 frases de forma randômica do t2\n",
        "print(\"Texto 2\")\n",
        "for i in range(3):\n",
        "    generated_text = generate_text(model, vocab, text2, max_length, context)\n",
        "    print(f\"Frase {i+1}: {generated_text}\\n\")\n",
        "\n",
        "# Gerar e imprimir 3 frases de forma randômica do t3\n",
        "print(\"Texto 3\")\n",
        "for i in range(3):\n",
        "    generated_text = generate_text(model, vocab, text3, max_length, context)\n",
        "    print(f\"Frase {i+1}: {generated_text}\\n\")\n",
        "\n",
        "# Gerar e imprimir 3 frases de forma randômica do t4\n",
        "print(\"Texto 4\")\n",
        "for i in range(3):\n",
        "    generated_text = generate_text(model, vocab, text4, max_length, context)\n",
        "    print(f\"Frase {i+1}: {generated_text}\\n\")\n",
        "\n",
        "    # Gerar e imprimir 3 frases de forma randômica do t5\n",
        "print(\"Texto 5\")\n",
        "for i in range(3):\n",
        "    generated_text = generate_text(model, vocab, text5, max_length, context)\n",
        "    print(f\"Frase {i+1}: {generated_text}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sXcKA_RalW9",
        "outputId": "fa5dd66c-30fe-4940-9c03-c98eaaf42c22"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto 1\n",
            "Frase 1: uma triste , [UNK] e eram nove horas da noite .\n",
            "\n",
            "Frase 2: uma triste , [UNK] e eram nove horas da noite , dizialhe me vir das três vezes . ora levantou me estava com\n",
            "\n",
            "Frase 3: uma triste , [UNK] e eram nove horas da noite mais , era que eu fosse . a s bem\n",
            "\n",
            "Texto 2\n",
            "Frase 1: luís alves sacudiu a cabeça e sorriu\n",
            "\n",
            "Frase 2: luís alves sacudiu a cabeça e sorriu . era a poesia , na mão fui dama sobre é ainda a cansado do mesmo à hora por causa de boa de a instituição , que vira vinha por mim então daquele\n",
            "\n",
            "Frase 3: luís alves sacudiu a cabeça e sorriu ao filho que a última indiferença\n",
            "\n",
            "Texto 3\n",
            "Frase 1: o golpe foi profundo ,\n",
            "\n",
            "Frase 2: o golpe foi profundo . quando o resto , dos\n",
            "\n",
            "Frase 3: o golpe foi profundo . às dois degraus com ela não levantava nomes tarde de\n",
            "\n",
            "Texto 4\n",
            "Frase 1: uma [UNK] de vinte anos , tal\n",
            "\n",
            "Frase 2: uma [UNK] de vinte anos cheia , a consulta e ele será eu me houve fora .\n",
            "\n",
            "Frase 3: uma [UNK] de vinte anos ;\n",
            "\n",
            "Texto 5\n",
            "Frase 1: [UNK] morre por tão pouco [UNK] , e pálida , moça , que ele\n",
            "\n",
            "Frase 2: [UNK] morre por tão pouco [UNK] impaciência sentimentos\n",
            "\n",
            "Frase 3: [UNK] morre por tão pouco [UNK] : acho , chegou\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
